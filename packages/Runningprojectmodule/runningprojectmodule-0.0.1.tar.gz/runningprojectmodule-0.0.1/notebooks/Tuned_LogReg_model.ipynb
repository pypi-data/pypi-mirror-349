{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3447beb",
   "metadata": {},
   "source": [
    "# Generating, training, and saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30865ede",
   "metadata": {},
   "source": [
    "### Order of operations for the code\n",
    "- The `main` function is called to run the entire file.\n",
    "    - Loads the data from a CSV file into a pandas DataFrame.\n",
    "    - Drops unnecessary columns from the DataFrame.\n",
    "    - Splits the data into training and testing sets based on athlete IDs.\n",
    "    - Calls the `run_exps` function to run multiple experiments.\n",
    "        - For each of n experiments:\n",
    "            - It separately calls the `preparedata` function.\n",
    "                - The function calls three other functions:\n",
    "                    - `getMeanStd` gets statistics for normalisation per athlete\n",
    "                    - `normalize2` normalises the data using the statistics.\n",
    "                    - `getBalancedSubset` generates a dataset with equal numbers of healthy and unhealthy events based os samples of the original training set.\n",
    "                - It then sets the values for X_train, y_train, X_test, y_test.\n",
    "            - Then, it calls the `train_model` function, which trains a logistic regressor based on some hyperparameters, and applies platt scaling to calibrate the model.\n",
    "            - Next, it evaluates the model for key metrics using either the `eval` function or else using the `vis_and_eval` function to produce some visualisations of model performance.\n",
    "            - Finally it selects the model with the highest recall for an accuracy of at least 0.65 (this was an important tradeoff during experimentation.)\n",
    "        - Once all experiments have been run it prints out the averages for metrics across all experiments, and returns the best model from across all experiments.\n",
    "    - The best model is then saved to a file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a260027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.680, Accuracy: 0.661, Recall(most important): 0.600\n",
      "ROC AUC: 0.694, Accuracy: 0.691, Recall(most important): 0.620\n",
      "ROC AUC: 0.704, Accuracy: 0.646, Recall(most important): 0.680\n",
      "ROC AUC: 0.680, Accuracy: 0.643, Recall(most important): 0.620\n",
      "ROC AUC: 0.696, Accuracy: 0.639, Recall(most important): 0.620\n",
      "(0.6613009198423128, 0.6, 0.6798496993987977)\n",
      "(0.6908672798948752, 0.62, 0.694188376753507)\n",
      "(0.6455321944809461, 0.68, 0.703941215764863)\n",
      "(0.6425755584756899, 0.62, 0.6796259185036739)\n",
      "(0.6389618922470434, 0.62, 0.6958650634602538)\n",
      "Mean Accuracy: 0.656\n",
      "Mean Recall: 0.63\n",
      "Mean ROC AUC: 0.691\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def normalize2(row, mean_df, std_df, athlete_id):\n",
    "    '''\n",
    "    Normalize the data using z-score normalization.\n",
    "    '''\n",
    "    mu = mean_df.loc[athlete_id]\n",
    "    su = std_df.loc[athlete_id]\n",
    "    z = (row - mu)/su\n",
    "    return z\n",
    "\n",
    "def getMeanStd(data):\n",
    "    mean = data[data['injury'] == 0].groupby('Athlete ID').mean()\n",
    "    std = data[data['injury'] == 0].groupby('Athlete ID').std()\n",
    "    std.replace(to_replace=0.0, value=0.01, inplace=True)\n",
    "    return mean, std\n",
    "\n",
    "def getBalancedSubset(X_train, samplesPerClass):\n",
    "    '''\n",
    "    Create a balanced subset of the data by sampling from each athlete's data.\n",
    "    '''\n",
    "    healthySet   = pd.DataFrame()\n",
    "    unhealthySet = pd.DataFrame()\n",
    "    \n",
    "    stats = pd.DataFrame(X_train[['Athlete ID','injury']].groupby(['Athlete ID','injury']).size().reset_index(name='counts'))\n",
    "    stats = pd.DataFrame(stats[['Athlete ID']].groupby(['Athlete ID']).size().reset_index(name='counts'))\n",
    "    stats.drop(stats[stats['counts'] < 2].index, inplace=True)\n",
    "    athleteList = stats['Athlete ID'].unique()\n",
    "\n",
    "    samplesPerAthlete = int(np.floor(samplesPerClass) / len(athleteList))\n",
    "\n",
    "    for athlete in athleteList:\n",
    "        if unhealthySet.empty:\n",
    "            unhealthySet = X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 0)].sample(samplesPerAthlete, replace=True)\n",
    "        else:\n",
    "            unhealthySet = pd.concat([unhealthySet, X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 0)].sample(samplesPerAthlete,replace=True)], ignore_index=True)\n",
    "        if healthySet.empty:\n",
    "            healthySet = X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 1)].sample(samplesPerAthlete, replace=True)\n",
    "        else:\n",
    "            healthySet = pd.concat([healthySet, X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 1)].sample(samplesPerAthlete,replace=True)], ignore_index=True)\n",
    "\n",
    "\n",
    "    balancedSet = pd.concat([healthySet, unhealthySet], ignore_index=True)\n",
    "    return balancedSet\n",
    "\n",
    "\n",
    "def preparedata(df,test_athletes):\n",
    "    '''\n",
    "    Prepare the data for training and testing.\n",
    "    This includes normalization, creating a balanced subset of the data, and splitting into training and testing sets.\n",
    "    '''\n",
    "    X_test_original = df[df['Athlete ID'].isin(test_athletes)].copy() # Keep a copy for normalization\n",
    "    X_train_original = df[~df['Athlete ID'].isin(test_athletes)].copy() # Keep a copy\n",
    "\n",
    "    X_train_means, X_train_std = getMeanStd(X_train_original)\n",
    "    X_test_means, X_test_std = getMeanStd(X_test_original)\n",
    "    X_train_balanced = getBalancedSubset(X_train_original.copy(), 2048)\n",
    "    \n",
    "    # Set target variable for testing and training\n",
    "    y_train = X_train_balanced['injury']\n",
    "    y_test = X_test_original['injury']\n",
    "\n",
    "    # Apply normalization to the balanced training data\n",
    "    X_train_norm = X_train_balanced.apply(lambda x: normalize2(x, X_train_means, X_train_std, x['Athlete ID']), axis=1)\n",
    "    X_train_norm = X_train_norm.drop(columns=['injury', 'Date', 'Athlete ID'], errors='ignore')\n",
    "\n",
    "    # Apply normalization to the test data using the testing statistics\n",
    "    # Note this is a source of data leakage! but the alternative is not feasible\n",
    "    X_test_norm = X_test_original.apply(lambda x: normalize2(x, X_test_means, X_test_std, x['Athlete ID']), axis=1)\n",
    "    X_test_norm = X_test_norm.drop(columns=['injury', 'Date', 'Athlete ID'], errors='ignore')\n",
    "\n",
    "\n",
    "    return y_train, y_test, X_train_norm, X_test_norm\n",
    "\n",
    "def train_model(X_train, y_train, **params):\n",
    "    '''\n",
    "    Train the model using the training data and some already tuned hyperparameters.\n",
    "    This includes applying Platt scaling for better probability estimates.\n",
    "    '''\n",
    "    # model = XGBClassifier()\n",
    "    # model = GaussianNB()\n",
    "\n",
    "    # Create and fit the logistic regression model\n",
    "    model = LogisticRegression(**params, max_iter=500, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Apply Platt scaling with cross-validation\n",
    "    calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=5)  # You can adjust cv as needed\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "    return calibrated_model\n",
    "\n",
    "def vis_and_eval(model, y_true, X, y_pred):\n",
    "    '''\n",
    "    Produce key evaluation metrics and visualisations.\n",
    "    '''\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, model.predict_proba(X)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall(most important): {recall:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(3,2))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # print ROC curve and AUC\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.plot(fpr, tpr, label='ROC Curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return (accuracy,recall,roc_auc)\n",
    "\n",
    "def eval(model, y_true, X, y_pred, **params):\n",
    "    '''\n",
    "    Produce key evaluation metrics without visualisations.\n",
    "    '''\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, model.predict_proba(X)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}, Accuracy: {accuracy:.3f}, Recall(most important): {recall:.3f}\")\n",
    "   \n",
    "    return (accuracy,recall,roc_auc)\n",
    "\n",
    "def run_exps(df,test_set, n =5, **params):\n",
    "    '''\n",
    "    Run experiments to determine the best model.\n",
    "    This includes training n models with the given parameters,\n",
    "    evaluating them, and printings some metrics for each one. \n",
    "    it then returns the best model based on recall.   \n",
    "        \n",
    "    '''\n",
    "    best_model = None\n",
    "    best_recall = 0\n",
    "    all_results = []\n",
    "    for i in range(n):\n",
    "        y_train, y_test, X_train, X_test = preparedata(df, test_set)\n",
    "        model = train_model(X_train, y_train,**params)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results = eval(model, y_test, X_test, y_pred)\n",
    "        # results = vis_and_eval(model, y_test, X_test, y_pred)\n",
    "        \n",
    "        if results[1] > best_recall and results[0] > 0.65:\n",
    "            best_model = model\n",
    "            best_recall = results[1]\n",
    "        all_results.append(results)\n",
    "    _ =[print(i) for i in all_results]\n",
    "    # rewrite three lines above as f strings with 3 decimal places\n",
    "    print(f\"Mean Accuracy: {np.mean([x[0] for x in all_results]):.3f}\")\n",
    "    print(f\"Mean Recall: {np.mean([x[1] for x in all_results]):.2f}\")\n",
    "    print(f\"Mean ROC AUC: {np.mean([x[2] for x in all_results]):.3f}\")\n",
    "    return best_model\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    '''\n",
    "    Main function to run the experiments.\n",
    "    '''\n",
    "\n",
    "    # Load, clean, and split the data\n",
    "    dfday = pd.read_csv('../data/raw/day_approach.csv')\n",
    "    dfday.drop(list(dfday.filter(regex = 'perceived|sprinting|strength')), axis = 1, inplace = True)\n",
    "    athletes = sorted(list(dfday['Athlete ID'].unique()))\n",
    "    test_athletes = athletes[len(athletes) - 10:]\n",
    "\n",
    "    # run experiments to determine the best model\n",
    "    best_model = run_exps(dfday, test_athletes, n = 5, C=0.01, penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "   \n",
    "    # Save the model to a file\n",
    "    with open('../models/logistic_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    \n",
    "    # Uncomment the following lines to run additional experiments with different hyperparameters\n",
    "    '''\n",
    "    print(\"-\" * 50)  # Separator for better readability\n",
    "    C_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "    l1_ratio_values = [0.2, 0.5, 0.8]\n",
    "    for C in C_values:\n",
    "        for l1_ratio in l1_ratio_values:\n",
    "            print(f\"Running experiments with C={C}, l1_ratio={l1_ratio}, penalty='elasticnet', solver='saga', class_weight='balanced'\")\n",
    "            run_exps(dfday, test_athletes, n = 3, C=C, penalty='elasticnet', solver='saga', l1_ratio=l1_ratio)\n",
    "            print(\"-\" * 50)  # Separator for better readability\n",
    "   '''\n",
    "   \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7a09",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "So, I have code that produces fairly well functioning models. I would like to evaluate the data for different testing sets, so I'm going to try to create different training and testing splits for the data. the resulting functionn is below. my results produced quite poor recall, and when outputting the traing and testing splits they were found to be incredibly uneven. I could revisit this, but I would have to really significantly redesign my method for . \n",
    "```python\n",
    "def run_exps(df,athletes, n =5):\n",
    "    all_results = []\n",
    "    for i in range(n):\n",
    "        test_athletes = np.random.choice(athletes, size=10, replace=False)\n",
    "        \n",
    "        y_train, y_test, X_train, X_test = preparedata(df, test_athletes)\n",
    "        \n",
    "        # print number of count of +ve and -ve samples in the training set\n",
    "        print(\"Training set counts: \", y_train[y_train==1].value_counts())\n",
    "        print(\"out of Training set counts: \", y_train.value_counts())\n",
    "        print(\"Testing set counts: \", y_test[y_test==1].value_counts())\n",
    "        print(\"out of Testing set counts: \", y_test.value_counts())\n",
    "        model = train_model(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results = eval(model, y_test, X_test, y_pred)\n",
    "        all_results.append(results)\n",
    "    _ =[print(i) for i in all_results]\n",
    "    print(\"Mean Accuracy: \", np.mean([x[0] for x in all_results]))\n",
    "    print(\"Mean Recall: \", np.mean([x[1] for x in all_results]))\n",
    "    print(\"Mean ROC AUC: \", np.mean([x[2] for x in all_results]))\n",
    "    \n",
    "    \n",
    "def main():\n",
    "\n",
    "    dfday = pd.read_csv('C:/Users/milo/Desktop/publicprojectsMilo/RunningVolume_Injury/data/raw/day_approach.csv')\n",
    "    dfday.drop(list(dfday.filter(regex = 'perceived|sprinting|strength')), axis = 1, inplace = True)\n",
    "    athletes = sorted(list(dfday['Athlete ID'].unique()))\n",
    "    # test_athletes = athletes[len(athletes) - 10:]\n",
    "    run_exps(dfday, athletes, n = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5a2e1",
   "metadata": {},
   "source": [
    "Then, can work on updating the current pipeline to transform both user data and pipeline data to a longer timeframe, including volume from 1 week or two weeks prior as well as from the past 7 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
