#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""dasconv tool

This file is part of dastools.

dastools is free software: you can redistribute it and/or modify it under the terms of the GNU
General Public License as published by the Free Software Foundation, either version 3 of the
License, or (at your option) any later version.

dastools is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without
even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
General Public License for more details.

You should have received a copy of the GNU General Public License along with this program. If
not, see https://www.gnu.org/licenses/.

   :Copyright:
       2021 Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences, Potsdam, Germany
   :License:
       GPLv3
   :Platform:
       Linux

.. moduleauthor:: Javier Quinteros <javier@gfz-potsdam.de>, GEOFON, GFZ Potsdam
"""

import argparse
import sys
import logging
import datetime
from time import perf_counter
import concurrent.futures
from obspy import Trace
from dastools.input import checkDASdata
from dastools.input.tdms import TDMS
from dastools.input.tdms import TDMSReader
from dastools.input.optodas import OptoDAS
from dastools.input.optodas import OptoDASReader
from dastools.input.das import NoData
from dastools import __version__
from dastools.utils import str2date
from dastools.utils import nslc
import dastools.output.archive as da
from dastools.partition.parallelization import SplitTDMS
from dastools.partition.parallelization import SplitOptoDAS
import inspect
from typing import Union
from typing import Type


def main():
    # Inspect the archive.py module to list the Classes based on Archive
    dictarchive = dict()
    for name, obj in inspect.getmembers(da):
        if inspect.isclass(obj):
            if issubclass(obj, da.Archive) and name != 'Archive':
                dictarchive[name] = obj

    helparchive = 'Structure to be used when saving the converted data. SDS: SeisComP Data Structure; '
    helparchive += 'StreamBased: one file per stream; StreamBasedHour: one file per stream per hour.\n'
    helparchive += 'Available options are [%s] (default: StreamBased)' % ', '.join(dictarchive.keys())

    dasclasses = ['OptoDAS', 'TDMS']

    # Check verbosity in the output
    msg = 'Read, manipulate and convert seismic waveforms generated by a DAS system.'
    parser = argparse.ArgumentParser(description=msg)
    parser.add_argument('-l', '--loglevel',
                        help='Verbosity in the output (default: INFO)',
                        choices=['CRITICAL', 'ERROR', 'WARNING', 'INFO',
                                 'DEBUG'],
                        default='INFO')
    parser.add_argument('--logout', help='Name of the log file (default: output.log)',
                        default='output.log', type=str)
    parser.add_argument('-d', '--directory',
                        help='Directory where files are located (default: ".")',
                        default='.')
    parser.add_argument('--start', '--starttime',
                        help='Start of the selected time window.\nFormat: 2019-02-01T00:01:02.123456Z',
                        default=None)
    parser.add_argument('--end', '--endtime',
                        help='End of the selected time window.\nFormat: 2019-02-01T00:01:02.123456Z',
                        default=None)
    parser.add_argument('--chstart', type=int,
                        help='First channel to export (default: 0)',
                        default=0)
    parser.add_argument('--chstop', type=int,
                        help='Last channel to export (default: last channel available)',
                        default=None)
    parser.add_argument('--chstep', type=int,
                        help='Step between channels in the selection (default: 1)',
                        default=1)
    parser.add_argument('--decimate', type=int, choices=[1, 5],
                        help='Factor by which the sampling rate is lowered by decimation (default: 1)',
                        default=1)
    parser.add_argument('-N', '--network',
                        help='Network code to store in the miniseed header (default: "XX")',
                        default='XX')
    parser.add_argument('-C', '--channel',
                        help='Channel code to store in the miniseed header (default: "HSF")',
                        default='HSF')
    parser.add_argument('-f', '--inputfmt', type=str, choices=dasclasses,
                        help='Format of the input files (default: auto detect)', default=None)
    parser.add_argument('-p', '--processes', type=int, choices=[1, 2, 4, 8, 16, 32],
                        help='Number of threads to spawn when parallelizing the conversion (default: 1)', default=1)
    parser.add_argument('-o', '--outstruct', type=str, choices=dictarchive.keys(),
                        help=helparchive, default='StreamBased')
    parser.add_argument('-V', '--version', action='version', version='dasconv v%s' % __version__)
    parser.add_argument('filename',
                        help='Experiment to read and process. It is usually the first part of the filenames.')

    args = parser.parse_args()
    logging.basicConfig(filename=args.logout, level=args.loglevel)

    logs = logging.getLogger('OpenFile')
    logs.setLevel(args.loglevel)

    dtstart = str2date(args.start)
    dtend = str2date(args.end)

    if dtend is not None and dtstart is not None and dtstart >= dtend:
        logs.error('End time is smaller than Start time.')
        return

    if len(args.network) != 2:
        logs.error('Network code must be two alphanumeric characters')
        return

    if len(args.channel) != 3:
        logs.error('Channel code must be three alphanumeric characters')
        return

    # If there are no input format try to guess it from the file extension filtering them with the parameters provided
    if args.inputfmt is None:
        # Check data format from the dataset (if any)
        clsdas = checkDASdata(args.filename, args.directory)
        if clsdas == TDMSReader:
            clsdas = TDMS
            splitdas = SplitTDMS
            inputfmt = 'TDMS'
        elif clsdas == OptoDASReader:
            clsdas = OptoDAS
            splitdas = SplitOptoDAS
            inputfmt = 'OptoDAS'
        else:
            logs.error('Input format cannot be guessed from the files found in the directory')
            sys.exit(-2)
    else:
        inputfmt = args.inputfmt
        if args.inputfmt == 'TDMS':
            clsdas = TDMS
            splitdas = SplitTDMS
        elif args.inputfmt == 'OptoDAS':
            clsdas = OptoDAS
            splitdas = SplitOptoDAS
        else:
            logs.error('Unknown input format.')
            sys.exit(-2)

    # First iterate through the files ('F')
    dasobj = clsdas(args.filename, directory=args.directory, iterate='F', networkcode=args.network,
                    channelcode=args.channel, starttime=dtstart, endtime=dtend, decimate=args.decimate,
                    channels=[x for x in range(args.chstart, args.chstop, args.chstep)],
                    loglevel='WARNING')

    # Split the work in tasks
    splt = splitdas(dasobj, args.outstruct, starttime=dtstart, endtime=dtend)
    tasks = splt.getbatch()
    # print('Tasks: %s' % tasks)
    logs.debug('Tasks: %s' % tasks)

    start_time = perf_counter()
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.processes) as executor:
        # Start the load operations and mark each future with its URL
        future_to_task = {executor.submit(task, args.filename, args.directory, inputfmt, args.network,
                                          args.channel, args.decimate, dictarchive[args.outstruct], args.loglevel,
                                          threadnum, params[0], params[1], params[2]): params for threadnum, params in enumerate(tasks)}

    end_time = perf_counter()
    logs.info('It took % 0.1f seconds to complete.' % (end_time - start_time))


def task(filename: str, directory: str, inputfmt: Type[Union[TDMS, OptoDAS]], networkcode: str, channelcode: str,
         decimate: int, outstructcls, loglevel: str, threadnum: int,
         channels: list, dtstart: Union[datetime.datetime, None], dtend: Union[datetime.datetime, None]):
    logging.info('Start task %d to store channels %s' % (threadnum, channels))
    iterate = 'D'
    try:
        dasobj = inputfmt(filename, directory=directory, iterate=iterate, channels=channels, networkcode=networkcode,
                          channelcode=channelcode, starttime=dtstart, endtime=dtend, decimate=decimate,
                          loglevel=loglevel)
    except NoData:
        logging.error('There seems to be no data under the experiment name provided')
        sys.exit(-1)
    except Exception:
        raise Exception('Unknown input format')

    # Selected archive structure
    # Archive files in current directory
    archive = outstructcls(root='.', experiment=filename, strictcheck=False)

    expectedtimes = dict()

    with dasobj:
        curstream = None

        # progress = tqdm(dasobj, total=dasobj.chunks(), position=threadnum)
        # for data in progress:
        for data in dasobj:
            # progress.set_postfix_str('(thrd: %d, sta:%s) %s' % (threadnum, data[1].station,
            #                                                     data[1].starttime.isoformat()))
            # logging.debug(data[1])

            if curstream != nslc(data[1]):
                # Save the previous Stream completely
                if curstream is not None:
                    archive.archive(tr0)
                    logging.info('Storing channel %s. Starttime: %s' % (curstream, data[1].get('starttime', None)))

                # Update which stream is being processed
                curstream = nslc(data[1])
                # Create the Trace
                tr0 = Trace(data=data[0], header=data[1])
            else:
                # Check if there is a gap in the signal. If a gap is detected, flush now
                if data[1]['starttime'] != expectedtimes[curstream]:
                    archive.archive(tr0)
                    tr0 = Trace(data=data[0], header=data[1])
                else:
                    tr0 += Trace(data=data[0], header=data[1])

            # Update the datetime of the expected sample
            expectedtimes[curstream] = data[1]['starttime'] + data[1]['npts']/data[1]['sampling_rate']
        else:
            logging.info('Storing last part of channel %s' % curstream)
            try:
                archive.archive(tr0)
            except KeyError:
                archive.archive(tr0)

            except UnboundLocalError:
                logging.error('Signal was not processed. Probably too short!')


if __name__ == '__main__':
    main()
