# Smart Chunker Engine: ТЗ (часть 3 из N)

**Официальный репозиторий:** [git@github.com:maverikod/vvz-smart-chunker-engine.git](git@github.com:maverikod/vvz-smart-chunker-engine.git)

> **Внимание!** Все операции с метаданными чанков (создание, хранение, экспорт, сериализация, статусы, метрики) должны выполняться исключительно через API и модели пакета [chunk_metadata_adapter](https://pypi.org/project/chunk-metadata-adapter/) (версии >=1.3.0). Форматы, сериализация, экспорт и валидация должны соответствовать [docs/implementation_plan.md](implementation_plan.md) и [docs/metadata_adapter_patch_spec.md](metadata_adapter_patch_spec.md).

## 3. Детали алгоритмов пост‑обработки

### 3.1 MERGE / SPLIT — жадная итеративная оптимизация

| Параметр                                  | Обозначение | Значение по умолчанию | Где задаётся                             |
| ----------------------------------------- | ----------- | --------------------- | ---------------------------------------- |
| Верхний порог схожести концов (`cos_sim`) | θ\_high     | **0.75**              | `config.yaml → thresholds.boundary_high` |
| Нижний порог когезии внутри               | θ\_low      | **0.35**              | `...boundary_low`                        |
| Балансный коэффициент                     | λ           | **0.4**               | `...lambda`                              |
| Минимум улучшения Score                   | ε           | **1e‑2**              | `...epsilon`                             |
| Макс. проходов                            | max\_iter   | **4**                 | `...max_iter`                            |

#### 3.1.1 Вычисление метрик

```
cohesion(i)   = mean_{p<q} cos(t_p, t_q)
boundary(i)   = cos( last(T_i), first(T_{i+1}) )
Score         = Σ_i ( cohesion(i) – λ·boundary(i) )
```

(см. `metrics.py`)

#### 3.1.2 Pseudocode (module `iterative_refine.py`)

```python
while True:            # ≤ max_iter
    improv = 0
    i = 0
    while i < len(chunks)-1:
        if boundary(i) > θ_high and gain_merge(i) > 0:
            merge(i)
            improv += Δ
            continue
        if cohesion(i) < θ_low and gain_split(i) > 0:
            split(i)
            improv += Δ
            i -= 1
        i += 1
    if improv < ε:
        break
```

### 3.2 TF‑IDF + POS‑тройки слой

1. **extract\_triples.py** → список троек ⟨N A V⟩ на чанк.
2. **triple\_cluster.py**:

   * SBERT → HDBSCAN → кластеры
   * weight = `1 + ln(1+size(cluster))`
3. **stats\_gate.py** решает, включать ли слой:

   * var\_tf, entropy, gini (см. Part 2).
4. **topic\_core.py** усредняет топ‑K лемм с учётом weight.

### 3.3 Метрики качества и теги

| Метрика              | Описание                            | JSON‑ключ                   |
| -------------------- | ----------------------------------- | --------------------------- |
| `coverage`           | 1 – cos(sentence\_emb, topic\_core) | `metrics.coverage`          |
| `cohesion`           | см. выше                            | `metrics.cohesion`          |
| `boundary_prev/next` | схожесть с соседями                 | `metrics.boundary_prev` ... |
| Тег `off_topic`      | coverage < 0.6                      | `tags[]`                    |

### 3.4 Продакшн-датасеты для тестирования SBERT-алгоритмов

Для валидации качества сегментации и MERGE/SPLIT-алгоритмов с использованием Sentence-BERT (SBERT) рекомендуется использовать открытые корпусные датасеты с реальными предложениями на русском и английском языках.

#### Рекомендуемые датасеты
- **Universal Dependencies (UD)**
  - English: [en_ewt-ud-test.conllu](https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu)
  - Russian: [ru_syntagrus-ud-test.conllu](https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu)

#### Автоматическая загрузка

В проекте реализован скрипт `scripts/data_downloader.py` (или `scripts/ud_downloader.py`), который скачивает эти датасеты в каталог `data/input/ud/`:

```bash
python scripts/data_downloader.py  # или python scripts/ud_downloader.py
```

#### Использование в тестах

Интеграционные тесты (см. `tests/test_iterative_refine_prod.py`) автоматически используют эти датасеты для проверки работы MERGE/SPLIT-алгоритма с реальными эмбеддингами SBERT. Тесты проверяют наличие файлов датасетов и работают с ними напрямую:

```python
@pytest.mark.parametrize('lang,model,fname', [
    ('en', 'all-MiniLM-L6-v2', 'data/input/ud/en_test.conllu'),
    ('ru', 'paraphrase-multilingual-MiniLM-L12-v2', 'data/input/ud/ru_test.conllu'),
    ('uk', 'paraphrase-multilingual-MiniLM-L12-v2', 'data/input/ud/uk_test.conllu'),
])
def test_refine_on_ud(lang, model, fname):
    assert os.path.exists(fname), f"Missing {fname}"
    # ... парсинг предложений и запуск IterativeRefiner ...
```

**Рекомендации:**
- Для ускорения тестов используйте ограничение на количество чанков (например, первые 100 предложений).
- Для новых языков добавьте соответствующие ссылки в скрипт загрузки и параметризацию теста.

---

> **См. также:**
> - [docs/implementation_plan.md](implementation_plan.md) — общий план, стандарты интеграции, требования к chunk_metadata_adapter
> - [docs/metadata_adapter_patch_spec.md](metadata_adapter_patch_spec.md) — спецификация расширения и патча для адаптера

Следующая часть (4): **API, схемы данных, пример конфигураций YAML**.
