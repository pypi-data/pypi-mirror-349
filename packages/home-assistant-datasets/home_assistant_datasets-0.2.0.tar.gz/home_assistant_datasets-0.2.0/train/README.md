# Assist LLM Training

## Dataset Creation

See generation/device-actions.ipynb for the script to create the dataset. The
dataset focuses on function calling for a fairly large context window and a
reasonably sized tool list as compared to other tool datasets that have
limited numbers of available tools.

## Training

The `llama31_unsloth.ipynb` [notebook](https://colab.research.google.com/github/allenporter/home-assistant-datasets/blob/main/train/llama31_unsloth.ipynb) can fine tune a LLama3.1 8B model
on the function calling dataset generated by the above notebook. This can be
run in a Google Colab T4 runtime.

## Serving

The model can be served from ollama. Unsloth has a [tutorial](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama) for exporting to Ollama
however it may not be using the latest ollama modelfile with updated prompts.

```bash
$ MODEL_DIR=/data/models
$ HUGGINGFACE_REPO=allenporter/assist-llm-GGUF
$ huggingface-cli download ${HUGGINGFACE_REPO} --exclude '*gguf' --local-dir=${MODEL_DIR}/assist-llm/
$ huggingface-cli download ${HUGGINGFACE_REPO} unsloth.Q4_K_M.gguf --local-dir=${MODEL_DIR}/assist-llm/
```

Run ollama server (in a separate tab):

```bash
$ OLLAMA_HOST=0.0.0.0 ./ollama serve
```

Create a modelfile:
```bash
$ ./ollama show llama3.1 --modelfile > ${MODEL_DIR}/assist-llm/Modelfile
```

Edit the modefile and replace the from line with `FROM ./unsloth.Q4_K_M.gguf`

```bash
$ OLLAMA_MODEL="allenporter/assist-llm"
$ ./ollama create ${OLLAMA_MODEL} --file ${MODEL_DIR}/assist-llm/Modelfile
$ ./ollama push ${OLLAMA_MODEL}
```
