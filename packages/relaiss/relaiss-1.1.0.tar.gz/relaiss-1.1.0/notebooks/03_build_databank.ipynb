{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ede68f0",
   "metadata": {},
   "source": [
    "# Building a New Dataset Bank for reLAISS\n",
    "### Authors: Evan Reynolds and Alex Gagliano\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to build a new dataset bank for reLAISS and use different feature combinations for nearest neighbor searches. The dataset bank is the foundation of reLAISS, containing all the features of transients that are used for similarity searches and anomaly detection.\n",
    "\n",
    "Building your own dataset bank allows you to incorporate new data, apply custom preprocessing steps, and tailor the feature set to your specific research needs.\n",
    "\n",
    "## Topics Covered\n",
    "1. Adding extinction corrections (A_V)\n",
    "2. Joining new lightcurve features\n",
    "3. Handling missing values\n",
    "4. Building the final dataset bank\n",
    "5. Using different feature combinations for nearest neighbor search:\n",
    "   - Lightcurve-only features\n",
    "   - Host-only features\n",
    "   - Custom feature subsets\n",
    "   - Feature weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212bf17",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and create the required directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff694547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from relaiss import constants\n",
    "import relaiss as rl\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('./figures', exist_ok=True)\n",
    "os.makedirs('./sfddata-master', exist_ok=True)\n",
    "\n",
    "# Define default feature sets from constants\n",
    "default_lc_features = constants.lc_features_const.copy()\n",
    "default_host_features = constants.host_features_const.copy()\n",
    "\n",
    "# Initialize client\n",
    "client = rl.ReLAISS()\n",
    "client.load_reference(\n",
    "    path_to_sfd_folder='./sfddata-master'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7229e7f",
   "metadata": {},
   "source": [
    "## 1. Adding Extinction Corrections (A_V)\n",
    "\n",
    "The first step in building a dataset bank is to add extinction corrections to account for interstellar dust. The Schlegel, Finkbeiner & Davis (SFD) dust maps are used to estimate the amount of extinction.\n",
    "\n",
    "```python\n",
    "# Example code for adding extinction corrections\n",
    "from sfdmap2 import sfdmap\n",
    "\n",
    "df = pd.read_csv(\"../data/large_df_bank.csv\")\n",
    "m = sfdmap.SFDMap('../data/sfddata-master')\n",
    "RV = 3.1  # Standard value for Milky Way\n",
    "ebv = m.ebv(df['ra'].values, df['dec'].values)\n",
    "df['A_V'] = RV * ebv\n",
    "df.to_csv(\"../data/large_df_bank_wAV.csv\", index=False)\n",
    "```\n",
    "\n",
    "This adds the A_V (extinction in V-band) column to your dataset, which will be used later in the feature processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ecbca",
   "metadata": {},
   "source": [
    "## 2. Joining New Lightcurve Features\n",
    "\n",
    "If you have additional features in a separate dataset, you can merge them with your existing bank:\n",
    "\n",
    "```python\n",
    "# Example code for joining features\n",
    "df_large = pd.read_csv(\"../data/large_df_bank_wAV.csv\")\n",
    "df_small = pd.read_csv(\"../data/small_df_bank_re_laiss.csv\")\n",
    "\n",
    "key = 'ztf_object_id'\n",
    "extra_features = [col for col in df_large.columns if col not in df_small.columns]\n",
    "\n",
    "merged_df = df_small.merge(df_large[[key] + extra_features], on=key, how='left')\n",
    "\n",
    "lc_feature_names = constants.lc_features_const.copy()\n",
    "host_feature_names = constants.host_features_const.copy()\n",
    "\n",
    "small_final_df = merged_df.replace([np.inf, -np.inf, -999], np.nan).dropna(subset=lc_feature_names + host_feature_names)\n",
    "\n",
    "small_final_df.to_csv(\"../data/small_hydrated_df_bank_re_laiss.csv\", index=False)\n",
    "```\n",
    "\n",
    "This merges additional features from a larger dataset into your working dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb3a25",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values\n",
    "\n",
    "Missing values in the dataset can cause problems during analysis. reLAISS uses KNN imputation to fill in missing values:\n",
    "\n",
    "```python\n",
    "# Example code for handling missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "raw_host_feature_names = constants.raw_host_features_const.copy()\n",
    "raw_dataset_bank = pd.read_csv('../data/large_df_bank_wAV.csv')\n",
    "\n",
    "X = raw_dataset_bank[lc_feature_names + raw_host_feature_names]\n",
    "feat_imputer = KNNImputer(weights='distance').fit(X)\n",
    "imputed_filt_arr = feat_imputer.transform(X)\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_filt_arr, columns=lc_feature_names + raw_host_feature_names)\n",
    "imputed_df.index = raw_dataset_bank.index\n",
    "raw_dataset_bank[lc_feature_names + raw_host_feature_names] = imputed_df\n",
    "\n",
    "imputed_df_bank = raw_dataset_bank\n",
    "```\n",
    "\n",
    "KNN imputation works by finding the k-nearest neighbors in feature space for samples with missing values and using their values to fill in the gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d9e9d",
   "metadata": {},
   "source": [
    "## 4. Building the Final Dataset Bank\n",
    "\n",
    "With all the preprocessing done, we can now build the final dataset bank using the `build_dataset_bank` function from reLAISS:\n",
    "\n",
    "```python\n",
    "# Example code for building the final dataset bank\n",
    "from relaiss.features import build_dataset_bank\n",
    "\n",
    "dataset_bank = build_dataset_bank(\n",
    "    raw_df_bank=imputed_df_bank,\n",
    "    av_in_raw_df_bank=True,\n",
    "    path_to_sfd_folder=\"../data/sfddata-master\",\n",
    "    building_entire_df_bank=True\n",
    ")\n",
    "\n",
    "# Clean and save final dataset\n",
    "final_dataset_bank = dataset_bank.replace(\n",
    "    [np.inf, -np.inf, -999], np.nan\n",
    ").dropna(subset=lc_feature_names + host_feature_names)\n",
    "\n",
    "final_dataset_bank.to_csv('../data/large_final_df_bank_new_lc_feats.csv', index=False)\n",
    "```\n",
    "\n",
    "This function applies additional processing to prepare the features for reLAISS, including normalization and other transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b7b19",
   "metadata": {},
   "source": [
    "## 5. Using Different Feature Combinations\n",
    "\n",
    "reLAISS allows you to customize which features are used for similarity search. This can be useful for studying the importance of different features and for tailoring the search to specific scientific questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb99200",
   "metadata": {},
   "source": [
    "### 5.1 Using Only Lightcurve Features\n",
    "\n",
    "You can perform a search using only lightcurve features, ignoring host galaxy properties. This is useful when:\n",
    "- You want to focus solely on the temporal evolution of the transient\n",
    "- Host data might be unreliable or missing\n",
    "- You're testing hypotheses about lightcurve-based classification\n",
    "\n",
    "Here's how to set up a lightcurve-only search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_only_client = rl.ReLAISS()\n",
    "lc_only_client.load_reference(\n",
    "    path_to_sfd_folder='./sfddata-master',\n",
    "    lc_features=default_lc_features,  # Use default lightcurve features\n",
    "    host_features=[],  # Empty list means no host features\n",
    ")\n",
    "\n",
    "# Find neighbors using only lightcurve features\n",
    "neighbors_df_lc_only = lc_only_client.find_neighbors(\n",
    "    ztf_object_id='ZTF21abbzjeq',\n",
    "    n=5,\n",
    "    plot=True,\n",
    "    save_figures=True,\n",
    "    path_to_figure_directory='./figures/lc_only'\n",
    ")\n",
    "print(\"\\nNearest neighbors using only lightcurve features:\")\n",
    "print(neighbors_df_lc_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b24e5",
   "metadata": {},
   "source": [
    "### 5.2 Using Only Host Features\n",
    "\n",
    "Alternatively, you can perform a search using only host galaxy features, ignoring the lightcurve properties. This approach is valuable when:\n",
    "- You're more interested in environmental effects on transients\n",
    "- You want to find transients in similar host galaxies\n",
    "- You're studying correlations between host properties and transient types\n",
    "\n",
    "Here's how to set up a host-only search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_only_client = rl.ReLAISS()\n",
    "host_only_client.load_reference(\n",
    "    path_to_sfd_folder='./sfddata-master',\n",
    "    lc_features=[],  # Empty list means no lightcurve features\n",
    "    host_features=default_host_features,  # Use default host features\n",
    ")\n",
    "\n",
    "# Find neighbors using only host features\n",
    "neighbors_df_host_only = host_only_client.find_neighbors(\n",
    "    ztf_object_id='ZTF21abbzjeq',\n",
    "    n=5,\n",
    "    plot=True,\n",
    "    save_figures=True,\n",
    "    path_to_figure_directory='./figures/host_only'\n",
    ")\n",
    "print(\"\\nNearest neighbors using only host features:\")\n",
    "print(neighbors_df_host_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe91d24",
   "metadata": {},
   "source": [
    "### 5.3 Using Custom Feature Subset\n",
    "\n",
    "You can also select specific features from both categories for a more targeted search. This allows you to:\n",
    "- Focus on the features most relevant to your research question\n",
    "- Reduce noise by excluding less useful features\n",
    "- Test hypotheses about which features drive similarity\n",
    "\n",
    "Here's how to create a custom feature subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c721c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific lightcurve and host features\n",
    "custom_lc_features = ['g_peak_mag', 'r_peak_mag', 'g_peak_time', 'r_peak_time']\n",
    "custom_host_features = ['host_ra', 'host_dec', 'gKronMag', 'rKronMag']\n",
    "\n",
    "custom_client = rl.ReLAISS()\n",
    "custom_client.load_reference(\n",
    "    path_to_sfd_folder='./sfddata-master',\n",
    "    lc_features=custom_lc_features,  # Custom subset of lightcurve features\n",
    "    host_features=custom_host_features,  # Custom subset of host features\n",
    ")\n",
    "\n",
    "# Find neighbors with custom feature subset\n",
    "neighbors_df_custom = custom_client.find_neighbors(\n",
    "    ztf_object_id='ZTF21abbzjeq',\n",
    "    n=5,\n",
    "    plot=True,\n",
    "    save_figures=True,\n",
    "    path_to_figure_directory='./figures/custom_features'\n",
    ")\n",
    "print(\"\\nNearest neighbors using custom feature subset:\")\n",
    "print(neighbors_df_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9ee34",
   "metadata": {},
   "source": [
    "### 5.4 Using Feature Weighting\n",
    "\n",
    "You can also adjust the relative importance of lightcurve features versus host galaxy features using the `weight_lc_feats_factor` parameter:\n",
    "- Values > 1: Emphasize lightcurve features\n",
    "- Values < 1: Emphasize host features\n",
    "- Value = 1: Equal weighting (default)\n",
    "\n",
    "This allows you to fine-tune the balance between photometric and host properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular search prioritizing lightcurve features\n",
    "neighbors_df_lc_weighted = client.find_neighbors(\n",
    "    ztf_object_id='ZTF21abbzjeq',\n",
    "    n=5,\n",
    "    weight_lc_feats_factor=3.0,  # Strongly prioritize lightcurve features\n",
    "    plot=True,\n",
    "    save_figures=True,\n",
    "    path_to_figure_directory='./figures/lc_weighted'\n",
    ")\n",
    "print(\"\\nNearest neighbors with lightcurve features weighted 3x:\")\n",
    "print(neighbors_df_lc_weighted)\n",
    "\n",
    "# Now prioritize host features by using a factor < 1\n",
    "neighbors_df_host_weighted = client.find_neighbors(\n",
    "    ztf_object_id='ZTF21abbzjeq',\n",
    "    n=5,\n",
    "    weight_lc_feats_factor=0.3,  # Prioritize host features\n",
    "    plot=True,\n",
    "    save_figures=True,\n",
    "    path_to_figure_directory='./figures/host_weighted'\n",
    ")\n",
    "print(\"\\nNearest neighbors with host features given higher weight:\")\n",
    "print(neighbors_df_host_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243999b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Building your own dataset bank and customizing feature combinations provides powerful flexibility for tailoring reLAISS to your specific research questions. By selecting different feature combinations and adjusting feature weights, you can explore various aspects of transient similarity and discover new insights about the transient population."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
