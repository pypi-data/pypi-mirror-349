# Oxford flowers dataset If it's not present, then it will be downloaded & set up
data: "~/datasets/Oxford_Flowers102/images"
image_size: 128

vqgan:
    # Parameters specific to VQGAN 
    # Note: the following sub-hierarchy is (i.e., model:, training:) 
    #     is not required; in fact it gets flattened.
    # Only used for user readability
    model:
        hidden_channels: 256
        num_downsamples: 3
        internal_dim: 128
        vq_embedding_dim: 4
        codebook_levels: 4
        vq_num_embeddings: 64
        commitment-weight: 0.5
        no_grad_ckpt: False 
    
    # training specific to VQGAN
    training:
        batch_size: 64
        learning_rate: 1e-3
        # warmup is before the adversarial training starts
        warmup_epochs: 5
        epochs: 2000
        # base learning rate gets scaled by the batch size
        base_lr: 1e-4
        # regularization parameters
        lambda_adv: 0.03
        lambda_ce: 0.0
        lambda_l1: 0.2
        lambda_mse: 0.5
        lambda_perc: 5e-5
        lambda_spec: 1e-3 
        lambda_vq: 0.25 

    wandb: 
        project_name: "flocoder-vqgan-flowers"
        # run_name: "best to set via CLI"


preencoding: 
    # the encoder is the bottleneck, so for speed, don't make batch_size too big or use too many workers
    batch_size: 32
    num_workers: 8
    max_storage_gb: 50
    augs_per: 768
    #vqgan_checkpoint: "checkpoints/final_vqgan_flowers.pt"
    vqgan_checkpoint: "SD"


flow: 
    # Parameters specific to flow model.
    # note: any encoding/decoding/visualization will likely read from the VQGAN section above
    model: 
        num_layers: 8
        num_blocks: 4
        num_heads: 8
        num_channels: 256
        num_residuals: 2
        n_classes: 102
        condition: true
    training: 
        batch_size: 2048
        learning_rate: 2e-4
        num_epochs: 10000
        # can add more later 

    wandb:
        project_name: "flocoder-flow-flowers"
        # run_name: "best to set via CLI"

