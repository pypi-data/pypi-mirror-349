{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Welcome to the Dell AI SDK Walkthrough\n",
                "\n",
                "In this interactive guide, we'll explore the capabilities of the Dell AI SDK, helping you understand how to harness Dell's enterprise-grade AI infrastructure for your projects. By the end of this notebook, you'll have hands-on experience with:\n",
                "\n",
                "- Setting up and authenticating your Dell AI client\n",
                "- Discovering and exploring available AI models\n",
                "- Understanding Dell's AI hardware platforms\n",
                "- Generating deployment configurations for production environments\n",
                "\n",
                "Let's begin our journey into enterprise AI deployment!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting Up Your Environment\n",
                "\n",
                "First, let's import the necessary libraries. The Dell AI SDK provides a simple client interface that handles all communication with Dell's AI infrastructure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dell_ai import DellAIClient"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Connecting to Dell AI\n",
                "\n",
                "Let's initialize our connection to Dell AI services. The client acts as your gateway to all Dell AI functionality.\n",
                "\n",
                "üí° **Authentication Tip:** The client will automatically try to use your Hugging Face token from the cache. If you haven't authenticated with Hugging Face before, you can pass your token directly as shown in the commented example below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the client (uses HF token from cache if available)\n",
                "client = DellAIClient()\n",
                "\n",
                "# If you need to provide a token directly:\n",
                "# client = DellAIClient(token=\"your_huggingface_token\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verifying Your Connection\n",
                "\n",
                "Before proceeding, let's make sure we're properly connected and authenticated with the Dell AI platform. This step is crucial as it verifies your access to Dell's enterprise AI services.\n",
                "\n",
                "Let's check your authentication status and retrieve your user information:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's verify our connection and see your account details\n",
                "is_auth = client.is_authenticated()\n",
                "print(f\"‚úÖ Authentication status: {'Successful' if is_auth else 'Failed'}\")\n",
                "\n",
                "if is_auth:\n",
                "    user_info = client.get_user_info()\n",
                "    print(\"\\nüìã Your Hugging Face User Information:\")\n",
                "    for key, value in user_info.items():\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "    print(\"\\nYou're all set to explore Dell's AI capabilities!\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Please check your authentication token and try again.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploring Available AI Models\n",
                "\n",
                "Now that we're connected, let's discover the AI models available through Dell's platform. Dell AI provides access to a curated selection of high-performance models optimized for enterprise use cases.\n",
                "\n",
                "These models range from large language models (LLMs) to specialized AI models for various tasks, all optimized to run efficiently on Dell's hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìö Found 30 models on the Dell AI platform\n",
                        "\n",
                        "Sample of available models:\n",
                        "  ‚Ä¢ meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "  ‚Ä¢ meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
                        "  ‚Ä¢ google/gemma-3-27b-it\n",
                        "  ‚Ä¢ google/gemma-3-12b-it\n",
                        "  ‚Ä¢ google/gemma-3-4b-it\n",
                        "\n",
                        "üîç Spotlight on: meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "  Description: The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
                        "  License: llama4\n",
                        "  Is Multimodal: True\n",
                        "\n",
                        "Tip: To explore a different model, use: client.get_model('model_name')\n"
                    ]
                }
            ],
            "source": [
                "# Let's explore the available models\n",
                "models = client.list_models()\n",
                "print(f\"üìö Found {len(models)} models on the Dell AI platform\")\n",
                "\n",
                "# Display a few examples\n",
                "print(\"\\nSample of available models:\")\n",
                "for model in models[:5]:  # Show first 5 models\n",
                "    print(f\"  ‚Ä¢ {model}\")\n",
                "\n",
                "# Show more details about one model\n",
                "if models:\n",
                "    example_model = models[0]  # Let's look at the first model in detail\n",
                "    model_details = client.get_model(example_model)\n",
                "\n",
                "    print(f\"\\nüîç Spotlight on: {example_model}\")\n",
                "    print(f\"  Description: {model_details.description}\")\n",
                "    print(f\"  License: {model_details.license}\")\n",
                "    print(f\"  Is Multimodal: {model_details.is_multimodal}\")\n",
                "\n",
                "    print(\"\\nTip: To explore a different model, use: client.get_model('model_name')\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Understanding Dell's AI Hardware Platforms\n",
                "\n",
                "One of Dell's key strengths is its range of optimized hardware platforms for AI workloads. These platforms are designed to deliver maximum performance for different types of AI models and use cases.\n",
                "\n",
                "Let's explore the available hardware platforms:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üñ•Ô∏è Found 7 AI-optimized hardware platforms\n",
                        "\n",
                        "Available platforms:\n",
                        "  ‚Ä¢ xe9680-nvidia-h200\n",
                        "  ‚Ä¢ xe9680-nvidia-h100\n",
                        "  ‚Ä¢ xe9680-amd-mi300x\n",
                        "  ‚Ä¢ xe9680-intel-gaudi3\n",
                        "  ‚Ä¢ xe8640-nvidia-h100\n",
                        "  ‚Ä¢ r760xa-nvidia-h100\n",
                        "  ‚Ä¢ r760xa-nvidia-l40s\n",
                        "\n",
                        "üîç Platform Details: XE9680 Nvidia H200\n",
                        "  Server: xe9680\n",
                        "  GPU Information:\n",
                        "    - Vendor: Nvidia\n",
                        "    - Type: H200\n",
                        "    - Memory per GPU: 141G\n",
                        "    - Total GPUs: 8\n"
                    ]
                }
            ],
            "source": [
                "# Discover Dell AI hardware platforms\n",
                "platforms = client.list_platforms()\n",
                "print(f\"üñ•Ô∏è Found {len(platforms)} AI-optimized hardware platforms\")\n",
                "\n",
                "# List all available platforms\n",
                "print(\"\\nAvailable platforms:\")\n",
                "for platform in platforms:\n",
                "    print(f\"  ‚Ä¢ {platform}\")\n",
                "\n",
                "# Deep dive into one platform\n",
                "if platforms:\n",
                "    example_platform = platforms[0]\n",
                "    platform_details = client.get_platform(example_platform)\n",
                "\n",
                "    print(f\"\\nüîç Platform Details: {platform_details.name}\")\n",
                "    print(f\"  Server: {platform_details.server}\")\n",
                "    print(\"  GPU Information:\")\n",
                "    print(f\"    - Vendor: {platform_details.vendor}\")\n",
                "    print(f\"    - Type: {platform_details.gputype}\")\n",
                "    print(f\"    - Memory per GPU: {platform_details.gpuram}\")\n",
                "    print(f\"    - Total GPUs: {platform_details.totalgpucount}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model-Platform Compatibility\n",
                "\n",
                "Not all models can run efficiently on all hardware. Dell AI provides detailed compatibility information to help you choose the right hardware for your AI workloads.\n",
                "\n",
                "Let's check which platforms support our example model and what configurations are available:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Platform Support for: meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "==================================================\n",
                        "This model can be deployed on 2 platform(s):\n",
                        "\n",
                        "üñ•Ô∏è xe9680-amd-mi300x\n",
                        "  Configuration Option 1:\n",
                        "    ‚Ä¢ Max Batch Prefill Tokens: 16484\n",
                        "    ‚Ä¢ Max Input Tokens: 16383\n",
                        "    ‚Ä¢ Max Total Tokens: 16384\n",
                        "    ‚Ä¢ Num Gpus: 8\n",
                        "\n",
                        "üñ•Ô∏è xe9680-nvidia-h200\n",
                        "  Configuration Option 1:\n",
                        "    ‚Ä¢ Max Batch Prefill Tokens: 8484\n",
                        "    ‚Ä¢ Max Input Tokens: 8383\n",
                        "    ‚Ä¢ Max Total Tokens: 8384\n",
                        "    ‚Ä¢ Num Gpus: 8\n",
                        "    ‚Ä¢ Max Concurrent Requests: 500\n"
                    ]
                }
            ],
            "source": [
                "# Check which platforms support our model\n",
                "\n",
                "if models:\n",
                "    model_id = models[0]  # Using our previous example model\n",
                "    model_details = client.get_model(model_id)\n",
                "\n",
                "    print(f\"üìä Platform Support for: {model_id}\")\n",
                "    print(\"=\" * 50)\n",
                "\n",
                "    if not model_details.configs_deploy:\n",
                "        print(\"‚ö†Ô∏è No deployment configurations available for this model.\")\n",
                "    else:\n",
                "        print(\n",
                "            f\"This model can be deployed on {len(model_details.configs_deploy)} platform(s):\"\n",
                "        )\n",
                "\n",
                "        for platform_id, configs in model_details.configs_deploy.items():\n",
                "            print(f\"\\nüñ•Ô∏è {platform_id}\")\n",
                "\n",
                "            for idx, config in enumerate(configs, 1):\n",
                "                print(f\"  Configuration Option {idx}:\")\n",
                "\n",
                "                # Get all attributes dynamically including any extra fields\n",
                "                config_dict = config.model_dump()\n",
                "                for key, value in config_dict.items():\n",
                "                    # Format the key to be more readable\n",
                "                    formatted_key = key.replace(\"_\", \" \").title()\n",
                "                    print(f\"    ‚Ä¢ {formatted_key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 List Compatible Platforms for Each Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for model in models:\n",
                "    if models:\n",
                "        model_id = model  # Using our previous example model\n",
                "        model_details = client.get_model(model_id)\n",
                "\n",
                "        print(\"=\" * 100)\n",
                "        print(f\"\\nüìä Platform Support for: {model_id}\")\n",
                "\n",
                "        if not model_details.configs_deploy:\n",
                "            print(\"‚ö†Ô∏è No deployment configurations available for this model.\")\n",
                "        else:\n",
                "            print(\n",
                "                f\"   This model can be deployed on {len(model_details.configs_deploy)} platform(s):\"\n",
                "            )\n",
                "\n",
                "            for platform_id, configs in model_details.configs_deploy.items():\n",
                "                print(f\"\\nüñ•Ô∏è {platform_id}\")\n",
                "\n",
                "                for idx, config in enumerate(configs, 0):\n",
                "                    print(f\"  Configuration Option {idx+1}:\")\n",
                "\n",
                "                    # Get all attributes dynamically including any extra fields\n",
                "                    config_dict = config.model_dump()\n",
                "                    for key, value in config_dict.items():\n",
                "                        # Format the key to be more readable\n",
                "                        formatted_key = key.replace(\"_\", \" \").title()\n",
                "                        print(f\"    ‚Ä¢ {formatted_key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 List Compatible Platforms for Specified Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check which platforms support our model\n",
                "user_input = input(\"Enter model id:üëâ \")\n",
                "\n",
                "if models:\n",
                "    # model_id = models[0]  # Using our previous example model\n",
                "    model_details = client.get_model(user_input)\n",
                "\n",
                "    print(f\"üìä Platform Support for: {user_input}\")\n",
                "    print(\"=\" * 50)\n",
                "\n",
                "    if not model_details.configs_deploy:\n",
                "        print(\"‚ö†Ô∏è No deployment configurations available for this model.\")\n",
                "    else:\n",
                "        print(\n",
                "            f\"This model can be deployed on {len(model_details.configs_deploy)} platform(s):\"\n",
                "        )\n",
                "\n",
                "        for platform_id, configs in model_details.configs_deploy.items():\n",
                "            print(f\"\\nüñ•Ô∏è {platform_id}\")\n",
                "\n",
                "            for idx, config in enumerate(configs, 1):\n",
                "                print(f\"  Configuration Option {idx}:\")\n",
                "\n",
                "                # Get all attributes dynamically including any extra fields\n",
                "                config_dict = config.model_dump()\n",
                "                for key, value in config_dict.items():\n",
                "                    # Format the key to be more readable\n",
                "                    formatted_key = key.replace(\"_\", \" \").title()\n",
                "                    print(f\"    ‚Ä¢ {formatted_key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Generating Model Deployment Configurations\n",
                "\n",
                "Now let's see how to deploy your chosen AI model! Dell AI simplifies deployment by generating ready-to-use configuration snippets for Docker and Kubernetes.\n",
                "\n",
                "Let's generate deployment snippets for our example model on a compatible platform:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Preparing Deployment for meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "On Platform: xe9680-amd-mi300x\n",
                        "\n",
                        "Deployment Configuration:\n",
                        "  ‚Ä¢ Max Batch Prefill Tokens: 16484\n",
                        "  ‚Ä¢ Max Input Tokens: 16383\n",
                        "  ‚Ä¢ Max Total Tokens: 16384\n",
                        "  ‚Ä¢ Num Gpus: 8\n",
                        "\n",
                        "üì¶ Docker Deployment Command:\n",
                        "Copy this command to deploy with Docker:\n",
                        "```bash\n",
                        "docker run \\\n",
                        "    -it \\\n",
                        "    -p 80:80 \\\n",
                        "    --security-opt seccomp=unconfined \\\n",
                        "    --device=/dev/kfd \\\n",
                        "    --device=/dev/dri \\\n",
                        "    --group-add video \\\n",
                        "    --ipc=host \\\n",
                        "    --shm-size 256g \\\n",
                        "    -e NUM_SHARD=8 \\\n",
                        "    -e MAX_BATCH_PREFILL_TOKENS=16484 \\\n",
                        "    -e MAX_TOTAL_TOKENS=16384 \\\n",
                        "    -e MAX_INPUT_TOKENS=16383 \\\n",
                        "    registry.dell.huggingface.co/enterprise-dell-inference-meta-llama-llama-4-maverick-17b-128e-instruct-amd\n",
                        "```\n",
                        "\n",
                        "‚ò∏Ô∏è Kubernetes Deployment Manifest:\n",
                        "```yaml\n",
                        "# Write the Kubernetes manifest below in a deployment.yaml file,\n",
                        "# and then run the following kubectl command on the Kubernetes Cluster:\n",
                        "# kubectl apply -f deployment.yaml\n",
                        "\n",
                        "apiVersion: apps/v1\n",
                        "kind: Deployment\n",
                        "metadata:\n",
                        "  name: tgi-deployment\n",
                        "spec:\n",
                        "  replicas: 1\n",
                        "  selector:\n",
                        "    matchLabels:\n",
                        "      app: tgi-server\n",
                        "  template:\n",
                        "    metadata:\n",
                        "      labels:\n",
                        "        app: tgi-server\n",
                        "        hf.co/model: meta-llama--Llama-4-Maverick-17B-128E-Instruct\n",
                        "        hf.co/task: text-generation\n",
                        "    spec:\n",
                        "      containers:\n",
                        "        - name: tgi-container\n",
                        "          image: registry.dell.huggingface.co/enterprise-dell-inference-meta-llama-llama-4-maverick-17b-128e-instruct-amd\n",
                        "          securityContext:\n",
                        "            seccompProfile:\n",
                        "              type: Unconfined\n",
                        "          resources:\n",
                        "            limits:\n",
                        "              amd.com/gpu: 8\n",
                        "          env: \n",
                        "            - name: NUM_SHARD\n",
                        "              value: \"8\"\n",
                        "            - name: MAX_BATCH_PREFILL_TOKENS\n",
                        "              value: \"16484\"\n",
                        "            - name: MAX_TOTAL_TOKENS\n",
                        "              value: \"16384\"\n",
                        "            - name: MAX_INPUT_TOKENS\n",
                        "              value: \"16383\"\n",
                        "          volumeMounts:\n",
                        "            - mountPath: /dev/shm\n",
                        "              name: dshm\n",
                        "            - name: dev-kfd\n",
                        "              mountPath: /dev/kfd\n",
                        "            - name: dev-dri\n",
                        "              mountPath: /dev/dri\n",
                        "      volumes:\n",
                        "        - name: dshm\n",
                        "          emptyDir:\n",
                        "            medium: Memory\n",
                        "            sizeLimit: 256Gi\n",
                        "        - name: dev-kfd\n",
                        "          hostPath:\n",
                        "            path: /dev/kfd\n",
                        "        - name: dev-dri\n",
                        "          hostPath:\n",
                        "            path: /dev/dri\n",
                        "---\n",
                        "apiVersion: v1\n",
                        "kind: Service\n",
                        "metadata:\n",
                        "  name: tgi-service\n",
                        "spec:\n",
                        "  type: LoadBalancer\n",
                        "  ports:\n",
                        "    - protocol: TCP\n",
                        "      port: 80\n",
                        "      targetPort: 80\n",
                        "  selector:\n",
                        "    app: tgi-server\n",
                        "---\n",
                        "apiVersion: networking.k8s.io/v1\n",
                        "kind: Ingress\n",
                        "metadata:\n",
                        "  name: tgi-ingress\n",
                        "  annotations:\n",
                        "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
                        "spec:\n",
                        "  ingressClassName: nginx-ingress\n",
                        "  rules:\n",
                        "    - http:\n",
                        "        paths:\n",
                        "          - path: /\n",
                        "            pathType: Prefix\n",
                        "            backend:\n",
                        "              service:\n",
                        "                name: tgi-service\n",
                        "                port:\n",
                        "                  number: 80\n",
                        "\n",
                        "```\n"
                    ]
                }
            ],
            "source": [
                "# Generate deployment snippets\n",
                "\n",
                "if models and model_details.configs_deploy:\n",
                "\n",
                "    model_id = model_details.repo_name\n",
                "\n",
                "    # Get the first platform ID from the available platforms\n",
                "    platform_ids = list(model_details.configs_deploy.keys())\n",
                "\n",
                "    if platform_ids:\n",
                "        platform_id = platform_ids[0]  # Take the first platform\n",
                "        config = model_details.configs_deploy[platform_id][\n",
                "            0\n",
                "        ]  # Take the first config of that platform\n",
                "\n",
                "        print(f\"üöÄ Preparing Deployment for {model_id}\")\n",
                "        print(f\"On Platform: {platform_id}\")\n",
                "        print(\"\\nDeployment Configuration:\")\n",
                "\n",
                "        # Display all config properties dynamically\n",
                "        config_dict = config.model_dump()\n",
                "        for key, value in config_dict.items():\n",
                "            # Format the key to be more readable\n",
                "            formatted_key = key.replace(\"_\", \" \").title()\n",
                "            print(f\"  ‚Ä¢ {formatted_key}: {value}\")\n",
                "\n",
                "    # Generate Docker deployment snippet\n",
                "    docker_snippet = client.get_deployment_snippet(\n",
                "        model_id=model_id,\n",
                "        platform_id=platform_id,\n",
                "        engine=\"docker\",\n",
                "        num_gpus=config.num_gpus,\n",
                "        num_replicas=1,\n",
                "    )\n",
                "\n",
                "    print(\"\\nüì¶ Docker Deployment Command:\")\n",
                "    print(\"Copy this command to deploy with Docker:\")\n",
                "    print(\"```bash\")\n",
                "    print(docker_snippet)\n",
                "    print(\"```\")\n",
                "\n",
                "    # Generate Kubernetes deployment snippet\n",
                "    k8s_snippet = client.get_deployment_snippet(\n",
                "        model_id=model_id,\n",
                "        platform_id=platform_id,\n",
                "        engine=\"kubernetes\",\n",
                "        num_gpus=config.num_gpus,\n",
                "        num_replicas=1,\n",
                "    )\n",
                "\n",
                "    print(\"\\n‚ò∏Ô∏è Kubernetes Deployment Manifest:\")\n",
                "    print(\"```yaml\")\n",
                "    print(k8s_snippet)\n",
                "    print(\"```\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No deployment configurations available. Try with a different model.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Exploring the App Catalog\n",
                "\n",
                "In addition to models, the Dell Enterprise Hub includes an App Catalog that provides ready-to-deploy applications optimized for AI workloads. In this section, we'll explore how to:\n",
                "\n",
                "1. List available applications\n",
                "2. Get detailed information about applications\n",
                "3. Explore configuration options\n",
                "4. Generate deployment snippets\n",
                "\n",
                "Let's start by listing the available applications:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìã Available Applications:\n",
                        "  1. OpenWebUI\n",
                        "  2. AnythingLLM\n"
                    ]
                }
            ],
            "source": [
                "# List available applications in the catalog\n",
                "print(\"üìã Available Applications:\")\n",
                "apps = client.list_apps()\n",
                "for i, app in enumerate(apps, 1):\n",
                "    print(f\"  {i}. {app}\")\n",
                "\n",
                "if not apps:\n",
                "    print(\"No applications available.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.1 Exploring Application Details\n",
                "\n",
                "Once you've identified an application of interest, you can retrieve its detailed information:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Getting details for app: openwebui\n",
                        "App: OpenWebUI\n",
                        "Description: OpenWebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to oper...\n",
                        "License: BSD-3-Clause\n",
                        "Documentation: https://docs.openwebui.com/\n",
                        "\n",
                        "Tags: chat, llm, multi-modal, mcp, model-management, vector-database, rag\n",
                        "Recommended Models: meta-llama/Llama-4-Scout-17B-16E-Instruct, google/gemma-3-27b-it...\n"
                    ]
                }
            ],
            "source": [
                "# Get details for an application (using first available app as an example)\n",
                "if apps:\n",
                "    app_id = apps[0].lower()  # Convert to lowercase for API compatibility\n",
                "\n",
                "    print(f\"\\nüîç Getting details for app: {app_id}\")\n",
                "    app_details = client.get_app(app_id)\n",
                "\n",
                "    print(f\"App: {app_details.name}\")\n",
                "    print(f\"Description: {app_details.description[:100]}...\")\n",
                "    print(f\"License: {app_details.license}\")\n",
                "    print(f\"Documentation: {app_details.docs}\")\n",
                "\n",
                "    # Display tags and recommended models\n",
                "    print(f\"\\nTags: {', '.join(app_details.tags)}\")\n",
                "    print(f\"Recommended Models: {', '.join(app_details.recommendedModels[:2])}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Understanding Configuration Options\n",
                "\n",
                "Each application has specific configuration options organized by component. \n",
                "\n",
                "**Note:** for a full list of configurable values, please refer to the [Dell Enterprise Hub Helm Chart repo](https://github.com/huggingface/dell-helm-chart).\n",
                "\n",
                "Let's explore these options:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚öôÔ∏è Available Configuration Options:\n",
                        "\n",
                        "Component 1: openwebui (Required: True)\n",
                        "Description: Core OpenWebUI web interface and API server\n",
                        "\n",
                        "Configuration Parameters:\n",
                        "  ‚Ä¢ STORAGE_CLASS_NAME: Storage class for persistent data\n",
                        "    Type: string, Required: True\n",
                        "    Default Value: gp2\n",
                        "    Helm Path: main.config.storageClassName\n",
                        "  ‚Ä¢ ENABLE_OPENAI_API: Enable OpenAI-compatible API\n",
                        "    Type: boolean, Required: False\n",
                        "    Default Value: False\n",
                        "    Helm Path: main.config.enableOpenAI\n",
                        "  ‚Ä¢ OPENAI_API_BASE_URLS: OpenAI API base URLs (semicolon-separated)\n",
                        "    Type: string, Required: False\n",
                        "    Default Value: \n",
                        "    Helm Path: main.config.openaiApiBaseUrls\n",
                        "\n",
                        "Secrets:\n",
                        "  ‚Ä¢ OPENAI_API_KEYS: OpenAI API keys (semicolon-separated)\n",
                        "    Type: string, Required: False\n",
                        "    Helm Path: main.secrets.openaiApiKeys\n",
                        "\n",
                        "Component 2: mcpo (Required: False)\n",
                        "Description: Model Context Protocol (MCP) proxy server for OpenWebUI\n",
                        "\n",
                        "Configuration Parameters:\n",
                        "  ‚Ä¢ ENABLE_MCPO_SERVER: Enable MCP proxy server deployment\n",
                        "    Type: boolean, Required: False\n",
                        "    Default Value: False\n",
                        "    Helm Path: mcpo.enabled\n",
                        "  ‚Ä¢ MCP_SERVER_CONFIG: JSON configuration for MCP servers\n",
                        "    Type: json, Required: False\n",
                        "    Default Value: {}\n",
                        "    Helm Path: mcpo.config.serverConfig\n"
                    ]
                }
            ],
            "source": [
                "# Explore configuration options\n",
                "if \"app_details\" in locals():\n",
                "    print(\"\\n‚öôÔ∏è Available Configuration Options:\")\n",
                "    config_options = []\n",
                "\n",
                "    for i, component in enumerate(app_details.components, 1):\n",
                "        print(f\"\\nComponent {i}: {component.name} (Required: {component.required})\")\n",
                "        print(f\"Description: {component.description}\")\n",
                "\n",
                "        if component.config:\n",
                "            print(\"\\nConfiguration Parameters:\")\n",
                "            for param in component.config:\n",
                "                print(f\"  ‚Ä¢ {param.name}: {param.description[:100]}\")\n",
                "                print(f\"    Type: {param.type}, Required: {param.required or False}\")\n",
                "                if param.default is not None:\n",
                "                    print(f\"    Default Value: {param.default}\")\n",
                "                print(f\"    Helm Path: {param.helmPath}\")\n",
                "\n",
                "                # Store config information for later use\n",
                "                config_options.append(\n",
                "                    {\n",
                "                        \"component\": component.name,\n",
                "                        \"param\": param,\n",
                "                    }\n",
                "                )\n",
                "\n",
                "        if component.secrets:\n",
                "            print(\"\\nSecrets:\")\n",
                "            for secret in component.secrets:\n",
                "                print(f\"  ‚Ä¢ {secret.name}: {secret.description[:100]}\")\n",
                "                print(f\"    Type: {secret.type}, Required: {secret.required or False}\")\n",
                "                print(f\"    Helm Path: {secret.helmPath}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.3 Creating a Deployment Configuration\n",
                "\n",
                "Based on the available options, we can create a custom configuration for our application deployment.\n",
                "In this example, we'll create a basic configuration using \"example\" parameters:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîß Creating Deployment Configuration\n",
                        "\n",
                        "üìù Example Configuration:\n",
                        "  1. main.config.storageClassName = gp2\n",
                        "  2. main.config.enableOpenAI = true\n",
                        "  3. main.config.openaiApiBaseUrls = http://first-api-endpoint/v1;https://second-api-endpoint/v1;https://third-api-endpoint/v1\n",
                        "  4. mcpo.enabled = true\n",
                        "  5. mcpo.config.serverConfig = {\n",
                        "  \"mcpServers\": {\n",
                        "    \"airbnb\": {\n",
                        "      \"command\": \"npx\",\n",
                        "      \"args\": [\n",
                        "        \"-y\",\n",
                        "        \"@openbnb/mcp-server-airbnb\",\n",
                        "        \"--ignore-robots-txt\"\n",
                        "      ]\n",
                        "    },\n",
                        "    \"time\": {\n",
                        "      \"command\": \"uvx\",\n",
                        "      \"args\": [\n",
                        "        \"mcp-server-time\",\n",
                        "        \"--local-timezone=America/New_York\"\n",
                        "      ]\n",
                        "    },\n",
                        "    \"gradio\": {\n",
                        "      \"url\": \"https://abidlabs-mcp-tools.hf.space/gradio_api/mcp/sse\"\n",
                        "    }\n",
                        "  }\n",
                        "}\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Create a configuration based on available options\n",
                "if \"app_details\" in locals() and app_details.components:\n",
                "    print(\"\\nüîß Creating Deployment Configuration\")\n",
                "\n",
                "    config = []\n",
                "\n",
                "    # Create configuration using example values from parameters\n",
                "    for opt in config_options:\n",
                "        param = opt[\"param\"]\n",
                "        # Use example value if available, otherwise use default or a type-appropriate value\n",
                "        if hasattr(param, \"example\") and param.example is not None:\n",
                "            value = param.example\n",
                "        else:\n",
                "            continue  # Skip if we can't determine a good value\n",
                "\n",
                "        config.append({\"helmPath\": param.helmPath, \"type\": param.type, \"value\": value})\n",
                "\n",
                "    # Display the final configuration\n",
                "    print(\"\\nüìù Example Configuration:\")\n",
                "    for i, cfg in enumerate(config, 1):\n",
                "        print(f\"  {i}. {cfg['helmPath']} = {cfg['value']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.4 Generating Application Deployment Snippets\n",
                "\n",
                "With our configuration defined, we can now generate a deployment snippet.\n",
                "This will produce a Helm command you can run to deploy the application:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üöÄ Generating deployment snippet...\n",
                        "\n",
                        "üì¶ Helm Deployment Command:\n",
                        "```bash\n",
                        "helm install my-openwebui deh/openwebui \\\n",
                        "  --set main.config.storageClassName=gp2 \\\n",
                        "  --set main.config.enableOpenAI=true \\\n",
                        "  --set main.config.openaiApiBaseUrls=http://first-api-endpoint/v1;https://second-api-endpoint/v1;https://third-api-endpoint/v1 \\\n",
                        "  --set mcpo.enabled=true \\\n",
                        "  --set-json 'mcpo.config.serverConfig={\"mcpServers\":{\"airbnb\":{\"command\":\"npx\",\"args\":[\"-y\",\"@openbnb/mcp-server-airbnb\",\"--ignore-robots-txt\"]},\"time\":{\"command\":\"uvx\",\"args\":[\"mcp-server-time\",\"--local-timezone=America/New_York\"]},\"gradio\":{\"url\":\"https://abidlabs-mcp-tools.hf.space/gradio_api/mcp/sse\"}}}'\n",
                        "```\n"
                    ]
                }
            ],
            "source": [
                "# Generate the deployment snippet\n",
                "if \"config\" in locals() and config:\n",
                "    print(\"\\nüöÄ Generating deployment snippet...\")\n",
                "    try:\n",
                "        snippet = client.get_app_snippet(app_id, config)\n",
                "\n",
                "        print(\"\\nüì¶ Helm Deployment Command:\")\n",
                "        print(\"```bash\")\n",
                "        print(snippet)\n",
                "        print(\"```\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error generating snippet: {e}\")\n",
                "else:\n",
                "    print(\"No valid configuration available to generate a snippet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Note: To run this command:**\n",
                "1. Make sure you have Helm installed (https://helm.sh/docs/intro/install/)\n",
                "2. Add the Dell Enterprise Hub chart repository:\n",
                "   ```\n",
                "   $ helm repo add deh https://huggingface.github.io/dell-helm-chart\n",
                "   $ helm repo update\n",
                "   ```\n",
                "3. Ensure your Kubernetes cluster is properly configured\n",
                "4. Run the command above to deploy OpenWebUI\n",
                "5. For detailed usage instructions, visit:\n",
                "   - OpenWebUI docs: https://docs.openwebui.com/\n",
                "   - Dell Enterprise Hub Helm charts: https://github.com/huggingface/dell-helm-chart"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "Congratulations! You've successfully completed the Dell AI SDK walkthrough. You now have the knowledge to:\n",
                "\n",
                "1. **Connect** to Dell's enterprise AI platform\n",
                "2. **Explore** available AI models, hardware platforms, and apps\n",
                "3. **Check** compatibility between models and platforms\n",
                "4. **Generate** deployment configurations for your chosen model or application\n",
                "\n",
                "### Where to go from here:\n",
                "\n",
                "- Try deploying one of these models or applications on your Dell hardware\n",
                "- Explore additional models and their capabilities\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
