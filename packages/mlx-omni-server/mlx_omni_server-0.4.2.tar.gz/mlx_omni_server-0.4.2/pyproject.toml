[project]
name = "mlx-omni-server"
version = "0.4.2"
description = "MLX Omni Server is a server that provides OpenAI-compatible APIs using Apple's MLX framework."
authors = [{ name = "madroid", email = "madroidmaq@gmail.com" }]
requires-python = ">=3.11"
readme = "README.md"
license = "MIT"
keywords = [
    "mlx",
    "ai",
    "agi",
    "aigc",
    "server",
    "openai",
    "tts",
    "stt",
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3.11",
]
dependencies = [
    "fastapi>=0.115.4,<0.116",
    "python-multipart>=0.0.20,<0.0.21",
    "pydantic>=2.9.2,<3",
    "f5-tts-mlx>=0.2.5,<0.3",
    "uvicorn>=0.34.0,<0.35",
    "numba>=0.57.0",
    "mlx-whisper>=0.4.1,<0.5",
    "mlx-lm>=0.24,<0.25",
    "huggingface-hub>=0.30",
    "mflux>=0.7.1,<0.8",
    "sse-starlette>=2.1.3,<3",
    "outlines>=0.1.11,<0.2",
    "aiohttp>=3.11.11,<4",
    "rich>=13.9.4",
    "openai>=1.78.0",
    "mlx-embeddings>=0.0.3",
    "mlx-audio>=0.2.2"
]

[project.urls]
Repository = "https://github.com/madroidmaq/mlx-omni-server"

[project.scripts]
mlx-omni-server = "mlx_omni_server.main:start"

[dependency-groups]
dev = [
    "pytest>=8.3.3,<9",
    "httpx>=0.27.2,<0.28",
    "pre-commit>=4.0.1,<5",
    "black>=24.10.0,<25",
    "isort>=5.13.2,<6",
    "weave>=0.51.46,<6",
]

[tool.hatch.build.targets.sdist]
include = ["src/mlx_omni_server"]

[tool.hatch.build.targets.wheel]
include = ["src/mlx_omni_server"]

[tool.hatch.build.targets.wheel.sources]
"src/mlx_omni_server" = "mlx_omni_server"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.pytest.ini_options]
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"
