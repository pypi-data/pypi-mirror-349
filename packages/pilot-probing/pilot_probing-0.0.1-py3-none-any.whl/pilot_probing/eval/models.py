from typing import Any, List, Optional, Union, Dict
from pydantic import BaseModel


class EvaluationDataExample(BaseModel):
    """
    Represents a single data payload transferred between SDK and server for evaluation.
    Attributes:
        example_id (str): A unique identifier for the example.
        messages (List[Dict[str, Any]]): A list of messages in the conversation.
        response (List[Dict[str, Any]]): A list of response generated by the language model.
        reference (Optional[List[Dict[str, Any]]]): An optional list of reference answers or expected outputs.
        score (Optional[Union[int, str]]): An optional score for the response. Defaults to None.
        analysis (Optional[str]): An optional analysis of the response. Defaults to None.
        confidence (Optional[float]): An optional confidence level for the evaluation. Defaults to None.
    """

    example_id: str
    messages: List[Dict[str, Any]]
    response: List[Dict[str, Any]]
    reference: Optional[List[Dict[str, Any]]] = None
    score: Optional[Union[int, str]] = None
    analysis: Optional[str] = None
    confidence: Optional[float] = None


class EvaluationDataset(BaseModel):
    """
    Represents a batch of data payloads transferred between SDK and server for evaluation.
    Attributes:
        examples (List[EvaluationPayload]): A list of individual evaluation payloads.
    """

    examples: List[EvaluationDataExample]


class EvaluationResult(BaseModel):
    """
    Represents the result of an evaluation, including the example_id,
    the generated reference, and the evaluation details.
    Attributes:
        example_id (str): A unique identifier for the example.
        reference (Optional[Any]): An optional reference answer or expected output. Defaults to None.
        score (Optional[Union[int, str]]): An optional score for the response. Defaults to None.
        analysis (Optional[str]): An optional analysis of the response. Defaults to None.
        confidence (Optional[float]): An optional confidence level for the evaluation. Defaults to None.
    """

    example_id: str
    reference: Optional[Any] = None
    score: Optional[Union[int, str]] = None
    analysis: Optional[str] = None
    confidence: Optional[float] = None

    @classmethod
    def from_evaluation_data(cls, evaluation_data: EvaluationDataExample) -> 'EvaluationResult':
        # Extract the first reference from the payload
        reference = evaluation_data.reference[0].get('content', None) if evaluation_data.reference else None
        # Create a new EvaluationResult object
        result = cls(
            example_id=evaluation_data.example_id,
            reference=reference,
            score=evaluation_data.score,
            analysis=evaluation_data.analysis,
            confidence=evaluation_data.confidence,
        )
        return result


class InputResponseExample(BaseModel):
    """
    Represents a single input-response example for evaluation.

    This class can handle two types of input structures:
    1. Direct input: Uses the `input` field.
    2. Prompt-based input: Uses `prompt` and `variables` fields. If `prompt` and `variables`
       are provided (and `input` is not), the `input` field will be automatically constructed
       by formatting the `prompt` with the `variables`.

    Attributes:
        example_id (str): A unique identifier for the example.
        input (Optional[str]): The input provided to the language model. Can be directly provided
                               or constructed from `prompt` and `variables`. Defaults to None.
        prompt (Optional[str]): The prompt template. Used if `input` is not provided. Defaults to None.
        variables (Optional[Dict[str, str]]): A dictionary of variables to fill into the prompt.
                                             Used if `input` is not provided. Defaults to None.
        response (str): The response generated by the language model.
        reference (Optional[str]): An optional reference answer or expected output. Defaults to None.
        score (Optional[Union[int, str]]): An optional score for the response. Defaults to None.
        analysis (Optional[str]): An optional analysis of the response. Defaults to None.
        confidence (Optional[float]): An optional confidence level for the evaluation. Defaults to None.
    """

    example_id: str
    input: Optional[str] = None
    response: str
    prompt: Optional[str] = None
    variables: Optional[Dict[str, str]] = None
    messages: Optional[List[Dict[str, str]]] = None
    reference: Optional[str] = None
    score: Optional[Union[int, str]] = None
    analysis: Optional[str] = None
    confidence: Optional[float] = None

    def to_evaluation_data(self) -> EvaluationDataExample:  # Renamed method and return type
        has_messages = self.messages is not None
        has_input = self.input is not None
        has_prompt = self.prompt is not None
        has_variables = self.variables is not None

        if has_messages and not any([has_input, has_prompt, has_variables]):
            if self.messages is None:  # Type safety
                raise ValueError('Messages is None despite has_messages being true.')
            # If messages are provided, use them as is without processing
            processed_messages = self.messages
        elif has_input and not any([has_prompt, has_variables]):
            if self.input is None:  # Type safety
                raise ValueError('Input is None despite has_input being true.')
            # If input is provided, use it as is without processing
            processed_messages = [
                {'role': 'user', 'content': self.input},
            ]
        elif has_prompt and has_variables:
            if self.prompt is None or self.variables is None:  # Type safety
                raise ValueError('Prompt or variables are None despite has_prompt/has_variables being true.')
            formatted_input = str(self.prompt)
            for key, value in self.variables.items():
                placeholder = '{{' + str(key) + '}}'
                formatted_input = formatted_input.replace(placeholder, str(value))
            processed_messages = [
                {'role': 'user', 'content': formatted_input},
            ]
        else:
            raise ValueError(
                'InputResponseExample must be convertible to a message list. '
                "Provide 'messages', or 'input', or both 'prompt' and 'variables'."
            )

        # Prepare reference for the payload
        # The payload expects List[Dict[str, str]] for reference
        payload_reference: Optional[List[Dict[str, str]]] = None
        if self.reference is not None:
            payload_reference = [{'role': 'assistant', 'content': self.reference}]

        return EvaluationDataExample(  # Use the new class name
            example_id=self.example_id,
            messages=processed_messages,
            response=[
                {'role': 'assistant', 'content': self.response},
            ],
            reference=payload_reference,
            score=self.score,
            analysis=self.analysis,
            confidence=self.confidence,
        )


class InputResponseDataset(BaseModel):
    """
    Input Response dataset model, encapsulating multiple InputResponseExamples.
    """

    examples: List[InputResponseExample]

    def to_evaluation_dataset(self) -> EvaluationDataset:
        """
        Convert the InputResponseDataset to an EvaluationDataset.
        """
        payloads = [example.to_evaluation_data() for example in self.examples]
        return EvaluationDataset(examples=payloads)


class Metric(BaseModel):
    """
    A prompt template for scoring, including evaluation criteria.
    """

    criteria: str
