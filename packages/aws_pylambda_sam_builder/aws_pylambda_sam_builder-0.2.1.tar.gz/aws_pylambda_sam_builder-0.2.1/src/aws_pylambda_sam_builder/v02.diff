4 files changed, 201 insertions(+), 134 deletions(-)
README.md                                |  10 +-
pyproject.toml                           |   3 +-
src/aws_pylambda_sam_builder/__init__.py | 166 +++++++++++++++++--------------
tests/test_unit.py                       | 156 ++++++++++++++++++-----------

modified   README.md
@@ -26,9 +26,15 @@ Resources:
       BuildMethod: makefile
 ```
 
-Now `sam build --build-in-source --cached  -t template.yaml YourLambda` uses per-package caching, doesn't need a container, and builds in 100ms after the first build.
+Now `sam build --build-in-source --parallel -t template.yaml YourLambda` uses per-package caching, doesn't need a container, and builds in 100ms after the first build.
 
 ### Issues
 * probably not safe for concurrent builds
 * can't build from source (only does binary wheels)
-* requires all transitive deps to be listed in requirements.txt. I did `poetry export -f initial_requirements.txt -o requirements.txt --without-hashes` 
\ No newline at end of file
+* requires all transitive deps to be listed in requirements.txt. I did `poetry export -f initial_requirements.txt -o requirements.txt --without-hashes` 
+
+
+### New in 0.2
+* Support for AWS Lambda python 3.13 environment
+* File lock for concurrent builds
+* Bugfix: previously, if a download failed, e.g. because no compiled package existed, a folder with the metadata hash would still be created. Then the next run would "succeed", but the dependency would not end up in the lambda zip. Current version makes sure to crash and delete the folder if downloading fails.
\ No newline at end of file
modified   pyproject.toml
@@ -1,6 +1,6 @@
 [tool.poetry]
 name = "aws_pylambda_sam_builder"
-version = "0.1.1"
+version = "0.2.0"
 description = "A tool for building AWS Lambda functions from Python projects."
 authors = ["Alex Loiko <alexandreloiko@gmail.com>"]
 license = "MIT"
@@ -13,6 +13,7 @@ aws-pylambda-sam-builder = "aws_pylambda_sam_builder:main"
 
 [tool.poetry.dependencies]
 python = ">=3.9"
+filelock = "^3.18.0"
 
 [tool.poetry.group.dev.dependencies]
 pytest = "^7.0"
modified   src/aws_pylambda_sam_builder/__init__.py
@@ -29,10 +29,11 @@ import argparse
 import hashlib
 import json
 import logging
-import os
 import subprocess
 import sys
-
+import shutil
+from filelock import FileLock
+from pathlib import Path
 from typing import NamedTuple
 
 __all__ = ["main"]
@@ -43,8 +44,8 @@ class BuildConfig(NamedTuple):
     abi: str
     implementation: str
     python_version: str
-    source: str
-    destination: str
+    source: Path
+    destination: Path
 
 def setup_logger():
     logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
@@ -57,30 +58,17 @@ def compute_hash(requirement: str, config: BuildConfig) -> str:
     key = f"{requirement.strip()}|{'.'.join(config.platform)}|{config.abi}|{config.implementation}|{config.python_version}"
     return hashlib.sha256(key.encode("utf-8")).hexdigest()
 
-def process_requirement(requirement: str, config: BuildConfig, cache_dir: str, logger: logging.Logger) -> str:
+def _download_and_unpack_wheel(requirement: str, config: BuildConfig, hash_dir: Path, logger: logging.Logger) -> None:
     """
-    Process a single requirement:
-      * Compute its hash.
-      * If not cached, download the wheel via pip and unpack it using unzip.
-      * Save metadata to the cache.
-    
-    Returns the cache folder path for this requirement.
+    Download the wheel using pip and unpack it.
     """
-    req_hash = compute_hash(requirement, config)
-    hash_dir = os.path.join(cache_dir, req_hash)
-    metadata_dir = os.path.join(hash_dir, "metadata")
-    unpacked_dir = os.path.join(hash_dir, "unpacked_wheel")
-
-    if os.path.exists(hash_dir):
-        logger.info("Using cached wheel for requirement: %s", requirement.strip())
-        return hash_dir
-
-    logger.info("Caching wheel for requirement: %s", requirement.strip())
-    os.makedirs(metadata_dir, exist_ok=True)
-    os.makedirs(unpacked_dir, exist_ok=True)
+    metadata_dir = hash_dir / "metadata"
+    unpacked_dir = hash_dir / "unpacked_wheel"
+    
+    metadata_dir.mkdir(parents=True, exist_ok=True)
+    unpacked_dir.mkdir(parents=True, exist_ok=True)
 
-    # Build the pip download command.
-    # Note: if --platform is a comma-separated list, pass as-is.
+    # Build the pip download command
     platform_args = []
     for platform in config.platform:
         platform_args.append("--platform")
@@ -94,7 +82,7 @@ def process_requirement(requirement: str, config: BuildConfig, cache_dir: str, l
         "--python-version", config.python_version,
         requirement.strip(),
         "--no-deps",
-        "-d", hash_dir,
+        "-d", str(hash_dir),
     ]
     try:
         subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
@@ -102,55 +90,88 @@ def process_requirement(requirement: str, config: BuildConfig, cache_dir: str, l
         logger.error("pip download failed for requirement %s", requirement.strip(), exc_info=e)
         sys.exit(1)
 
-    # Look for the downloaded wheel file.
-    wheel_files = [f for f in os.listdir(hash_dir) if f.endswith(".whl")]
+    # Look for the downloaded wheel file
+    wheel_files = list(hash_dir.glob("*.whl"))
     if not wheel_files:
         logger.error("No wheel file found for requirement %s", requirement.strip())
         sys.exit(1)
-    (wheel_file,) = wheel_files
-    wheel_file = os.path.join(hash_dir, wheel_file)
+    
+    wheel_file = wheel_files[0]
     logger.info("Unpacking wheel: %s", wheel_file)
-    subprocess.run(["unzip", "-o", wheel_file, "-d", unpacked_dir],
-                   check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+    
+    subprocess.run(["unzip", "-o", str(wheel_file), "-d", str(unpacked_dir)],
+                  check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
 
-    # Save metadata: include the original requirement and architecture fields.
+    # Save metadata
     metadata = {
         "requirement": requirement.strip(),
         "platform": config.platform,
         "abi": config.abi,
         "implementation": config.implementation,
         "python_version": config.python_version,
-        "wheel_file": wheel_file
+        "wheel_file": str(wheel_file)
     }
-    metadata_file = os.path.join(metadata_dir, "metadata.json")
-    with open(metadata_file, "w") as f:
-        json.dump(metadata, f)
-    logger.info("Cached wheel for %s at %s", requirement.strip(), hash_dir)
+    metadata_file = metadata_dir / "metadata.json"
+    metadata_file.write_text(json.dumps(metadata))
 
+def process_requirement(requirement: str, config: BuildConfig, cache_dir: Path, logger: logging.Logger) -> Path:
+    """
+    Process a single requirement:
+      * Compute its hash.
+      * If not cached, download the wheel via pip and unpack it using unzip.
+      * Save metadata to the cache.
+    
+    Returns the cache folder path for this requirement.
+    """
+    req_hash = compute_hash(requirement, config)
+    hash_dir = cache_dir / req_hash
+    lock_file = cache_dir / f"{req_hash}.lock"
+    
+    # Quick check if hash_dir already exists (fast path)
+    if hash_dir.exists():
+        logger.info("Using cached wheel for requirement: %s", requirement.strip())
+        return hash_dir
+    
+    # Slow path with lock
+    logger.info("Attempting to cache wheel for requirement: %s", requirement.strip())
+    
+    with FileLock(str(lock_file)):
+        # Check again after acquiring the lock (another process might have created it)
+        if hash_dir.exists():
+            logger.info("Another process created the cache for: %s", requirement.strip())
+            return hash_dir
+        
+        logger.info("Caching wheel for requirement: %s", requirement.strip())
+        try:
+            _download_and_unpack_wheel(requirement, config, hash_dir, logger)
+            logger.info("Cached wheel for %s at %s", requirement.strip(), hash_dir)
+        except Exception as e:
+            logger.error("Error caching wheel for %s: %s", requirement.strip(), str(e))
+            shutil.rmtree(hash_dir)
+            raise e
+    
     return hash_dir
 
-def symlink_directory_contents(src_dir: str, dest_dir: str, logger: logging.Logger) -> None:
+def symlink_directory_contents(src_dir: Path, dest_dir: Path, logger: logging.Logger) -> None:
     """
     Create symlinks in the destination directory for every file/directory in src_dir.
     """
-    if not os.path.exists(dest_dir):
-        os.makedirs(dest_dir, exist_ok=True)
-    for item in os.listdir(src_dir):
-        src_item = os.path.join(src_dir, item)
-        dest_item = os.path.join(dest_dir, item)
-        if os.path.lexists(dest_item):
-            os.remove(dest_item)
+    dest_dir.mkdir(parents=True, exist_ok=True)
+    for item in src_dir.iterdir():
+        dest_item = dest_dir / item.name
+        if dest_item.exists():
+            dest_item.unlink()
         try:
-            os.symlink(src_item, dest_item)
-            logger.debug("Symlinked %s -> %s", src_item, dest_item)
+            dest_item.symlink_to(item)
+            logger.debug("Symlinked %s -> %s", item, dest_item)
         except Exception as e:
-            logger.error("Failed to symlink %s to %s", src_item, dest_item, exc_info=e)
+            logger.error("Failed to symlink %s to %s", item, dest_item, exc_info=e)
             sys.exit(1)
 
 def main():
     parser = argparse.ArgumentParser(description="AWS PyLambda SAM Builder")
-    parser.add_argument("--aws-runtime", required=True, choices=["py310", "py311", "py312"], 
-                        help="Target AWS Lambda Python runtime (py310, py311, py312)")
+    parser.add_argument("--aws-runtime", required=True, choices=["py310", "py311", "py312", "py313"], 
+                        help="Target AWS Lambda Python runtime (py310, py311, py312, py313)")
     parser.add_argument("--aws-architecture", required=True, choices=["x86_64", "arm64"],
                         help="Target AWS Lambda architecture (x86_64, arm64)")
     parser.add_argument("--source", required=True, help="Source project directory")
@@ -169,6 +190,7 @@ def main():
         "py310": {"python_version": "3.10", "abi": "cp310"},
         "py311": {"python_version": "3.11", "abi": "cp311"},
         "py312": {"python_version": "3.12", "abi": "cp312"},
+        "py313": {"python_version": "3.13", "abi": "cp313"},
     }
     
     # Map architecture to platform
@@ -184,25 +206,24 @@ def main():
         abi=runtime_info["abi"],
         implementation="cp",  # Always "cp" for CPython
         python_version=runtime_info["python_version"],
-        source=args.source,
-        destination=args.destination,
+        source=Path(args.source),
+        destination=Path(args.destination),
     )
 
     # Set up the global cache directory.
-    cache_dir = os.path.expanduser("~/.cache/aws_pylambda_sam_builder")
-    os.makedirs(cache_dir, exist_ok=True)
+    cache_dir = Path.home() / ".cache" / "aws_pylambda_sam_builder"
+    cache_dir.mkdir(parents=True, exist_ok=True)
 
     # Read the requirements.txt from the source directory.
-    req_file = os.path.join(config.source, "requirements.txt")
-    if not os.path.exists(req_file):
+    req_file = config.source / "requirements.txt"
+    if not req_file.exists():
         logger.error("requirements.txt not found in source directory: %s", config.source)
         sys.exit(1)
 
     try:
-        with open(req_file, "r") as f:
-            # Skip empty lines and comments.
-            requirements = [line.strip() for line in f if line.strip() and not line.strip().startswith("#")]
-            requirements = [line.split(";")[0] for line in requirements]
+        requirements = [line.strip() for line in req_file.read_text().splitlines() 
+                      if line.strip() and not line.strip().startswith("#")]
+        requirements = [line.split(";")[0] for line in requirements]
     except Exception as e:
         logger.error("Error reading requirements.txt", exc_info=e)
         sys.exit(1)
@@ -216,8 +237,8 @@ def main():
     # Symlink each requirement's unpacked wheel into the destination directory.
     logger.info("Symlinking requirement wheels to destination: %s", config.destination)
     for cache_folder in cached_dirs:
-        unpacked = os.path.join(cache_folder, "unpacked_wheel")
-        if os.path.exists(unpacked):
+        unpacked = cache_folder / "unpacked_wheel"
+        if unpacked.exists():
             symlink_directory_contents(unpacked, config.destination, logger)
         else:
             logger.error("Unpacked wheel folder missing in cache: %s", cache_folder)
@@ -225,20 +246,17 @@ def main():
 
     # Symlink the project files (excluding requirements.txt) to the destination.
     logger.info("Symlinking project files to destination: %s", config.destination)
-    for item in os.listdir(config.source):
-        if item == "requirements.txt":
+    for item in config.source.iterdir():
+        if item.name == "requirements.txt":
             continue
-        src_item = os.path.join(config.source, item)
-        # Convert to absolute path to ensure symlinks are absolute
-        src_item_abs = os.path.abspath(src_item)
-        dest_item = os.path.join(config.destination, item)
-        if os.path.lexists(dest_item):
-            os.remove(dest_item)
+        dest_item = config.destination / item.name
+        if dest_item.exists():
+            dest_item.unlink()
         try:
-            os.symlink(src_item_abs, dest_item)
-            logger.debug("Symlinked project file %s -> %s", src_item_abs, dest_item)
+            dest_item.symlink_to(item)
+            logger.debug("Symlinked project file %s -> %s", item, dest_item)
         except Exception as e:
-            logger.error("Failed to symlink project file %s to %s", src_item_abs, dest_item, exc_info=e)
+            logger.error("Failed to symlink project file %s to %s", item, dest_item, exc_info=e)
             sys.exit(1)
 
     logger.info("Build completed successfully.")
modified   tests/test_unit.py
@@ -1,13 +1,10 @@
 # AI-generated tests, probably crashes at any modification :-(
-import os
-import sys
 import json
 import pytest
-from unittest.mock import patch, mock_open, MagicMock
 from pathlib import Path
-
-# Add the src directory to the path so we can import the package
-#sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
+from unittest.mock import patch, mock_open, MagicMock, Mock
+import subprocess
+import shutil
 
 from aws_pylambda_sam_builder import (
     compute_hash, 
@@ -25,14 +22,14 @@ def build_config():
         abi="cp311",
         implementation="cp",
         python_version="3.11",
-        source="/fake/source",
-        destination="/fake/destination"
+        source=Path("/fake/source"),
+        destination=Path("/fake/destination")
     )
 
 @pytest.fixture
 def mock_cache_dir(monkeypatch, tmp_path):
     """Create a temporary directory for the cache"""
-    monkeypatch.setattr("os.path.expanduser", lambda path: str(tmp_path) if "~" in path else path)
+    monkeypatch.setattr("pathlib.Path.home", lambda: tmp_path)
     return tmp_path
 
 # Test compute_hash function
@@ -52,100 +49,145 @@ def test_compute_hash(build_config):
     assert compute_hash(different_req, build_config) != hash_value
 
 # Test process_requirement function with existing cache
-@patch("os.path.exists")
-@patch("os.makedirs")
-def test_process_requirement_cached(mock_makedirs, mock_exists, build_config):
+@patch("pathlib.Path.exists")
+@patch("pathlib.Path.mkdir")
+def test_process_requirement_cached(mock_mkdir, mock_exists, build_config):
     mock_exists.return_value = True
     
     requirement = "requests==2.28.1"
-    cache_dir = "/fake/cache"
+    cache_dir = Path("/fake/cache")
     logger = MagicMock()
     
     result = process_requirement(requirement, build_config, cache_dir, logger)
     
     # Verify correct hash directory is returned
     hash_value = compute_hash(requirement, build_config)
-    expected_dir = f"/fake/cache/{hash_value}"
+    expected_dir = Path("/fake/cache") / hash_value
     assert result == expected_dir
     
     # Verify log message
     logger.info.assert_called_with("Using cached wheel for requirement: %s", requirement.strip())
     
     # Verify no directories were created
-    mock_makedirs.assert_not_called()
+    mock_mkdir.assert_not_called()
 
 # Test process_requirement function with new wheel
-@patch("os.path.exists")
-@patch("os.makedirs")
-@patch("os.listdir")
+@patch("pathlib.Path.exists")
+@patch("pathlib.Path.mkdir")
+@patch("pathlib.Path.glob")
 @patch("subprocess.run")
-@patch("builtins.open", new_callable=mock_open)
-def test_process_requirement_new_wheel(mock_file, mock_run, mock_listdir, 
-                                      mock_makedirs, mock_exists, build_config):
+@patch("pathlib.Path.write_text")
+@patch("aws_pylambda_sam_builder.FileLock")
+def test_process_requirement_new_wheel(mock_filelock, mock_write_text, mock_run, mock_glob, 
+                                      mock_mkdir, mock_exists, build_config):
     # Set up mocks
     mock_exists.return_value = False
-    mock_listdir.return_value = ["package-1.0-py3-none-any.whl"]
+    mock_glob.return_value = [Path("package-1.0-py3-none-any.whl")]
+    
+    # Set up FileLock mock to be used as a context manager
+    mock_lock_instance = MagicMock()
+    mock_filelock.return_value = mock_lock_instance
     
     requirement = "package==1.0"
-    cache_dir = "/fake/cache"
+    cache_dir = Path("/fake/cache")
     logger = MagicMock()
     
     result = process_requirement(requirement, build_config, cache_dir, logger)
     
     # Verify correct hash directory is returned
     hash_value = compute_hash(requirement, build_config)
-    expected_dir = f"/fake/cache/{hash_value}"
+    expected_dir = Path("/fake/cache") / hash_value
     assert result == expected_dir
     
     # Verify directories were created
-    assert mock_makedirs.call_count == 2
+    assert mock_mkdir.call_count == 2
     
     # Verify pip command was run
     assert mock_run.call_count == 2
     
     # Verify metadata was saved
-    mock_file.assert_called()
+    mock_write_text.assert_called()
+
+# Test process_requirement function with error after directory creation
+@patch("pathlib.Path.exists")
+@patch("pathlib.Path.mkdir")
+@patch("pathlib.Path.glob")
+@patch("subprocess.run")
+@patch("pathlib.Path.write_text")
+@patch("aws_pylambda_sam_builder.FileLock")
+@patch("shutil.rmtree")
+def test_process_requirement_error_cleanup(mock_rmtree, mock_filelock, mock_write_text, mock_run, 
+                                         mock_glob, mock_mkdir, mock_exists, build_config):
+    # Set up mocks
+    mock_exists.return_value = False
+    mock_glob.return_value = [Path("package-1.0-py3-none-any.whl")]
+    
+    # Set up FileLock mock to be used as a context manager
+    mock_lock_instance = MagicMock()
+    mock_filelock.return_value = mock_lock_instance
+    
+    # Make subprocess.run raise an exception after first successful call
+    mock_run.side_effect = [
+        MagicMock(),  # First call succeeds (pip download)
+        subprocess.CalledProcessError(1, "unzip", "Failed to unzip")  # Second call fails
+    ]
+    
+    requirement = "package==1.0"
+    cache_dir = Path("/fake/cache")
+    logger = MagicMock()
+    
+    # The function should raise the exception
+    with pytest.raises(subprocess.CalledProcessError):
+        process_requirement(requirement, build_config, cache_dir, logger)
+    
+    # Verify directories were created
+    assert mock_mkdir.call_count == 2
+    
+    # Verify cleanup was attempted
+    hash_value = compute_hash(requirement, build_config)
+    expected_dir = cache_dir / hash_value
+    mock_rmtree.assert_called_once_with(expected_dir)
+    
+    # Verify error was logged
+    logger.error.assert_called_once()
 
 # Test symlink_directory_contents function
-@patch("os.path.exists")
-@patch("os.makedirs")
-@patch("os.listdir")
-@patch("os.path.lexists")
-@patch("os.remove")
-@patch("os.symlink")
-def test_symlink_directory_contents(mock_symlink, mock_remove, mock_lexists, 
-                                   mock_listdir, mock_makedirs, mock_exists):
+@patch("pathlib.Path.exists")
+@patch("pathlib.Path.mkdir")
+@patch("pathlib.Path.iterdir")
+@patch("pathlib.Path.unlink")
+@patch("pathlib.Path.symlink_to")
+def test_symlink_directory_contents(mock_symlink_to, mock_unlink, mock_iterdir, 
+                                   mock_mkdir, mock_exists):
     mock_exists.return_value = True
-    mock_lexists.return_value = True
-    mock_listdir.return_value = ["file1.py", "file2.py"]
+    mock_iterdir.return_value = [Path("file1.py"), Path("file2.py")]
     
-    src_dir = "/fake/source"
-    dest_dir = "/fake/destination"
+    src_dir = Path("/fake/source")
+    dest_dir = Path("/fake/destination")
     logger = MagicMock()
     
     symlink_directory_contents(src_dir, dest_dir, logger)
     
     # Verify existing files were removed
-    assert mock_remove.call_count == 2
+    assert mock_unlink.call_count == 2
     
     # Verify symlinks were created
-    assert mock_symlink.call_count == 2
+    assert mock_symlink_to.call_count == 2
     
     # Check symlink calls
-    mock_symlink.assert_any_call("/fake/source/file1.py", "/fake/destination/file1.py")
-    mock_symlink.assert_any_call("/fake/source/file2.py", "/fake/destination/file2.py")
+    mock_symlink_to.assert_any_call(Path("file1.py"))
+    mock_symlink_to.assert_any_call(Path("file2.py"))
 
 # Test main function with mocks
 @patch("argparse.ArgumentParser")
-@patch("os.path.exists")
-@patch("os.makedirs")
-@patch("os.listdir")
-@patch("os.path.lexists")
-@patch("os.remove")
-@patch("os.symlink")
-@patch("builtins.open", new_callable=mock_open, read_data="requests==2.28.1\nflask==2.0.1\n")
-def test_main(mock_file, mock_symlink, mock_remove, mock_lexists, 
-             mock_listdir, mock_makedirs, mock_exists, mock_parser):
+@patch("pathlib.Path.exists")
+@patch("pathlib.Path.mkdir")
+@patch("pathlib.Path.iterdir")
+@patch("pathlib.Path.unlink")
+@patch("pathlib.Path.symlink_to")
+@patch("pathlib.Path.read_text")
+def test_main(mock_read_text, mock_symlink_to, mock_unlink, mock_iterdir, 
+             mock_mkdir, mock_exists, mock_parser):
     # Set up ArgumentParser mock
     parser_instance = mock_parser.return_value
     parser_instance.parse_args.return_value = MagicMock(
@@ -156,16 +198,16 @@ def test_main(mock_file, mock_symlink, mock_remove, mock_lexists,
     )
     
     mock_exists.return_value = True
-    mock_lexists.return_value = False
-    mock_listdir.side_effect = [
-        ["package-1.0-py3-none-any.whl"],  # For the wheel directory
-        ["unpacked_wheel"],  # For the cache directory contents
-        ["file1.py", "file2.py", "requirements.txt"]  # For the source directory
+    mock_read_text.return_value = "requests==2.28.1\nflask==2.0.1\n"
+    mock_iterdir.side_effect = [
+        [Path("package-1.0-py3-none-any.whl")],  # For the wheel directory
+        [Path("unpacked_wheel")],  # For the cache directory contents
+        [Path("file1.py"), Path("file2.py"), Path("requirements.txt")]  # For the source directory
     ]
     
     # Need to patch process_requirement separately
     with patch("aws_pylambda_sam_builder.process_requirement") as mock_process:
-        mock_process.side_effect = ["/fake/cache/hash1", "/fake/cache/hash2"]
+        mock_process.side_effect = [Path("/fake/cache/hash1"), Path("/fake/cache/hash2")]
         
         # Run the main function
         main()

