Metadata-Version: 2.4
Name: lazymodels
Version: 0.1.0
Summary: Smart lazy loader for Transformers with memory-based unloading (CPU/GPU aware)
Author: Nikita
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: transformers
Requires-Dist: torch
Requires-Dist: psutil
Dynamic: author
Dynamic: description
Dynamic: description-content-type
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# lazymodels

**LazyModels** is a lightweight utility for lazy-loading HuggingFace Transformers and managing memory automatically, both on CPU and GPU.

It keeps only the models you need in memory and unloads older ones when memory limits are exceeded. Works out of the box in one line of code:

```python
from lazymodels import lazy_model_transformer

model = lazy_model_transformer("gpt2")
````

## üî• Features

* ‚úÖ Lazy loading of models by name
* ‚úÖ Smart task detection: causal-lm, seq2seq, token classification, etc.
* ‚úÖ Automatic unloading when RAM or VRAM usage exceeds the limit
* ‚úÖ One global manager to control all memory
* ‚úÖ Transformers support out of the box

## üì¶ Example

```python
from lazymodels import lazy_model_transformer

# Load a causal LM model
model = lazy_model_transformer("gpt2")

# Load a seq2seq model with tokenizer
model2, tokenizer = lazy_model_transformer("t5-small", tokenizer=True)
```

## ‚öôÔ∏è Requirements

* `torch`
* `transformers`
* `psutil`

---

Licensed under MIT.
