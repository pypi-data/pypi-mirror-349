Metadata-Version: 2.4
Name: asflow
Version: 0.0.5
Summary: A lightweight, asynchronous workflow runner for ETL pipelines
Project-URL: Documentation, https://asflow.readthedocs.io/en/latest/
Project-URL: Repository, https://github.com/k24d/asflow
License-Expression: MIT
License-File: LICENSE
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: >=3.11
Requires-Dist: rich>=14.0.0
Description-Content-Type: text/markdown

# AsFlow

**AsFlow** (short for "Async workflow") is a lightweight, asynchronous workflow runner built in pure Python. It's designed for ETL (Extractâ€“Transformâ€“Load) pipelines with a focus on minimal setup, fast iteration, and seamless integration with tools like [Polars](https://pola.rs) and [Streamlit](https://streamlit.io).

![ScreenRecording](https://raw.githubusercontent.com/k24d/asflow/main/docs/assets/img/ScreenRecording.webp)

### Key Features

- ğŸ Pure Python, single-process â€” no external scheduler or services required
- âš™ï¸ Asynchronous by design â€” built on `asyncio` for parallel, non-blocking execution
- ğŸ“Š Rich console output â€” powered by [Rich](https://rich.readthedocs.io) for clean logs and progress bars
- ğŸ”„ Built for data engineering â€” integrates naturally with [Daft](https://www.getdaft.io), [DuckDB](https://duckdb.org), and [Polars](https://pola.rs)

## Installation

```
% pip install asflow
```

## Quick Start

**AsFlow** lets you build ETL workflows using standard `async` Python functions. Just add the `@flow` and `@flow.task` decoratorsâ€”everything else behaves like regular Python.

Hereâ€™s a minimal example:

```python
import asyncio
import duckdb
from asflow import Flow

flow = Flow(verbose=True)

# Extract: simulate saving raw data
@flow.task(on="words/*.jsonl.gz")
async def extract(word):
    await asyncio.sleep(1)  # Simulate I/O-bound operation
    flow.task.write({"word": word, "count": 1})

# Transform: read raw files into DuckDB
@flow.task
def transform():
    return duckdb.read_json("words/*.jsonl.gz")

# Define the workflow
@flow
async def main():
    words = ["Hello", "World"]

    # Run extractions concurrently
    async with asyncio.TaskGroup() as tg:
        for word in words:
            tg.create_task(extract(word))

    print(transform())

if __name__ == "__main__":
    asyncio.run(main())
```

Running this script produces output like:

```console
% python main.py
[12:34:56] Task extract('Hello') finished in 1.00s
           Task extract('World') finished in 1.01s
           Task transform() finished in 0.00s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚  word   â”‚ count â”‚
â”‚ varchar â”‚ int64 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hello   â”‚     1 â”‚
â”‚ World   â”‚     1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Whatâ€™s Happening?

- `extract()` simulates downloading or generating raw data. Itâ€™s async, so multiple calls run in parallel.
- `transform()` uses DuckDB to load the saved data into a queryable table.
- Results from `extract()` are written to disk (`words/*.jsonl.gz`), so theyâ€™re skipped on future runs if the file already existsâ€”making your workflow faster and more reliable.

### How AsFlow Helps
- âœ… Native support for both **synchronous and asynchronous tasks**
- ğŸ” Built-in **retries** for transient failures
- ğŸ§µ Configurable **concurrency limits** to avoid API throttling
- ğŸ“¦ Persistent storage of **raw data** (e.g., JSON, CSV, text)
- ğŸ—œï¸ Automatic support for **compressed files** like `.gz` and `.zst`
- ğŸ“Š **Rich-powered** logging with progress bars and status indicators

## Documentation

- [Overview](https://asflow.readthedocs.io/en/latest/)
- [Tutorial](https://asflow.readthedocs.io/en/latest/tutorial/)
