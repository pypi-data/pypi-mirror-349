import pytest
from shekar.preprocessing import (
    PunctuationNormalizer,
    AlphabetNormalizer,
    NumericNormalizer,
    SpacingStandardizer,
    EmojiRemover,
    EmailMasker,
    URLMasker,
    DiacriticsRemover,
    NonPersianRemover,
    HTMLTagRemover,
    RedundantCharacterRemover,
    ArabicUnicodeNormalizer,
    StopWordRemover,
    PunctuationRemover,
    MentionRemover,
    HashtagRemover,
    PunctuationSpacingStandardizer,
)


def test_correct_spacings():
    spacing_standardizer = SpacingStandardizer()

    input_text = "   Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡   Ù†Ù…ÙˆÙ†Ù‡   Ø§Ø³Øª. "
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª."
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¬Ø§Ø³ØªØŸØªÙˆ Ù…ÛŒØ¯ÙˆÙ†ÛŒØŸÙ†Ù…ÛŒØ¯ÙˆÙ†Ù…!"
    expected_output = "Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¬Ø§Ø³ØªØŸØªÙˆ Ù…ÛŒØ¯ÙˆÙ†ÛŒØŸÙ†Ù…ÛŒØ¯ÙˆÙ†Ù…!"
    assert spacing_standardizer.fit_transform(input_text) == expected_output

    input_text = "Ù†Ø§ØµØ± Ú¯ÙØª:Â«Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….Â»"
    expected_output = "Ù†Ø§ØµØ± Ú¯ÙØª:Â«Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….Â»"
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒ Ø²Ù†ÛŒØŸ"
    expected_output = "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒ Ø²Ù†ÛŒØŸ"
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….ØªÙˆ Ù†Ù…ÛŒâ€ŒØ¢ÛŒÛŒØŸ"
    expected_output = "Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….ØªÙˆ Ù†Ù…ÛŒâ€ŒØ¢ÛŒÛŒØŸ"
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ø¨Ù‡ Ù†Ú©ØªÙ‡ Ø±ÛŒØ²ÛŒ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ÛŒ!"
    expected_output = "Ø¨Ù‡ Ù†Ú©ØªÙ‡ Ø±ÛŒØ²ÛŒ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ÛŒ!"
    assert spacing_standardizer.fit_transform(input_text) == expected_output

    sentences = ["   Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡   Ù†Ù…ÙˆÙ†Ù‡   Ø§Ø³Øª. ", "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒ Ø²Ù†ÛŒØŸ"]
    expected_output = ["Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª.", "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒ Ø²Ù†ÛŒØŸ"]
    assert list(spacing_standardizer(sentences)) == expected_output
    assert list(spacing_standardizer.fit_transform(sentences)) == expected_output

    input_text = 13.4
    expected_output = "Input must be a string or a Iterable of strings."
    with pytest.raises(ValueError, match=expected_output):
        spacing_standardizer(input_text)


def test_remove_extra_spaces():
    spacing_standardizer = SpacingStandardizer()

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©  Ø¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©\n\n\nØ¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ©\n\nØ¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†\u200cÛŒÚ©\u200cØ¢Ø²Ù…ÙˆÙ†\u200cØ§Ø³Øª"
    expected_output = "Ø§ÛŒÙ†\u200cÛŒÚ©\u200cØ¢Ø²Ù…ÙˆÙ†\u200cØ§Ø³Øª"
    assert spacing_standardizer.fit_transform(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†\u200c ÛŒÚ©\u200c Ø¢Ø²Ù…ÙˆÙ†\u200c Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_standardizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©  Ø¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª  "
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_standardizer.fit_transform(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©  Ø¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª\n\n\n\n"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_standardizer(input_text) == expected_output


def test_mask_email():
    email_masker = EmailMasker(mask="")

    input_text = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†: she.kar@shekar.panir.io"
    expected_output = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†: "
    assert email_masker(input_text) == expected_output

    input_text = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†: she+kar@she-kar.io"
    expected_output = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†: "
    assert email_masker.fit_transform(input_text) == expected_output


def test_mask_url():
    url_masker = URLMasker(mask="")

    input_text = "Ù„ÛŒÙ†Ú©: https://shekar.parsi-shekar.com"
    expected_output = "Ù„ÛŒÙ†Ú©: "
    assert url_masker(input_text) == expected_output

    input_text = "Ù„ÛŒÙ†Ú©: http://shekar2qand.com/id=2"
    expected_output = "Ù„ÛŒÙ†Ú©: "
    assert url_masker.fit_transform(input_text) == expected_output


def test_normalize_numbers():
    numeric_normalizer = NumericNormalizer()
    input_text = "Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù© â’•34"
    expected_output = "Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹ Û±Û´Û³Û´"
    assert numeric_normalizer(input_text) == expected_output

    input_text = "Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Û°"
    expected_output = "Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Û°"
    assert numeric_normalizer.fit_transform(input_text) == expected_output


def test_unify_characters():
    alphabet_normalizer = AlphabetNormalizer()

    input_text = "Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ø©"
    expected_output = "Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø¯Ø±Ø¨Ø§Ø±Û€ Ù…Ø§"
    expected_output = "Ø¯Ø±Ø¨Ø§Ø±Û€ Ù…Ø§"
    assert alphabet_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ù†Ø§Ù…Û€ ÙØ±Ù‡Ù†Ú¯Ø³ØªØ§Ù†"
    expected_output = "Ù†Ø§Ù…Û€ ÙØ±Ù‡Ù†Ú¯Ø³ØªØ§Ù†"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø±Ø¦Ø§Ù„ÛŒØ³Ù… Ø±Ø¦ÛŒØ³ Ù„Ø¦ÛŒÙ…"
    expected_output = "Ø±Ø¦Ø§Ù„ÛŒØ³Ù… Ø±Ø¦ÛŒØ³ Ù„Ø¦ÛŒÙ…"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø±Ø£Ø³ Ù…ØªÙ„Ø£Ù„Ø¦ Ù…Ø£Ø®Ø°"
    expected_output = "Ø±Ø£Ø³ Ù…ØªÙ„Ø£Ù„Ø¦ Ù…Ø£Ø®Ø°"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ù…Ø¤Ù„Ù Ù…Ø¤Ù…Ù† Ù…Ø¤Ø³Ø³Ù‡"
    expected_output = "Ù…Ø¤Ù„Ù Ù…Ø¤Ù…Ù† Ù…Ø¤Ø³Ø³Ù‡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø¬Ø²Ø¡"
    expected_output = "Ø¬Ø²Ø¡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø³Ø§ÛŒØ©"
    expected_output = "Ø³Ø§ÛŒÙ‡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Û¿Ø¯Ù Ù…Ø§ Ø»Ù…Ú« Ø¨Ûƒ ÛÚªÚ‰ÙŠÚ±Ú• Ø¥ÚšÙ¼"
    expected_output = "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª"
    assert alphabet_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ú©Ø§Ø±ØªÙˆÙ†"
    expected_output = "Ú©Ø§Ø±ØªÙˆÙ†"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ù‡Ù…Ù‡ Ø¨Ø§ Ù‡Ù… Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ù¾Ù„ÛŒØ¯ÛŒ Ùˆ Ø³ØªÙ… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø§ÛŒØ³ØªØ§Ø¯"
    expected_output = "Ù‡Ù…Ù‡ Ø¨Ø§ Ù‡Ù… Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ù¾Ù„ÛŒØ¯ÛŒ Ùˆ Ø³ØªÙ… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø§ÛŒØ³ØªØ§Ø¯"
    assert alphabet_normalizer(input_text) == expected_output


def test_unify_punctuations():
    punct_normalizer = PunctuationNormalizer()

    input_text = "ØŸ?ØŒÙ¬!%:Â«Â»Ø›"
    expected_output = "ØŸØŸØŒØŒ!Ùª:Â«Â»Ø›"
    assert punct_normalizer(input_text) == expected_output

    input_text = "Ø³Ù„Ø§Ù…!Ú†Ø·ÙˆØ±ÛŒ?"
    expected_output = "Ø³Ù„Ø§Ù…!Ú†Ø·ÙˆØ±ÛŒØŸ"
    assert punct_normalizer.fit_transform(input_text) == expected_output


def test_unify_arabic_unicode():
    arabic_unicode_normalizer = ArabicUnicodeNormalizer()

    input_text = "ï·½"
    expected_output = "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…"
    assert arabic_unicode_normalizer(input_text) == expected_output

    input_text = "Ù¾Ù†Ø¬Ø§Ù‡ Ù‡Ø²Ø§Ø± ï·¼"
    expected_output = "Ù¾Ù†Ø¬Ø§Ù‡ Ù‡Ø²Ø§Ø± Ø±ÛŒØ§Ù„"
    assert arabic_unicode_normalizer(input_text) == expected_output

    input_text = "ï·² Ø§Ø¹Ù„Ù…"
    expected_output = "Ø§Ù„Ù„Ù‡ Ø§Ø¹Ù„Ù…"
    assert arabic_unicode_normalizer.fit_transform(input_text) == expected_output

    input_text = "ï·² ï·³"
    expected_output = "Ø§Ù„Ù„Ù‡ Ø§Ú©Ø¨Ø±"
    assert arabic_unicode_normalizer(input_text) == expected_output

    input_text = "ï·´"
    expected_output = "Ù…Ø­Ù…Ø¯"
    assert arabic_unicode_normalizer.fit_transform(input_text) == expected_output


def test_remove_punctuations():
    punc_remover = PunctuationRemover()

    input_text = "Ø§ØµÙÙ‡Ø§Ù†ØŒ Ù†ØµÙ Ø¬Ù‡Ø§Ù†!"
    expected_output = "Ø§ØµÙÙ‡Ø§Ù† Ù†ØµÙ Ø¬Ù‡Ø§Ù†"
    assert punc_remover(input_text) == expected_output

    input_text = "ÙØ±Ø¯ÙˆØ³ÛŒØŒ Ø´Ø§Ø¹Ø± Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø³Øª."
    expected_output = "ÙØ±Ø¯ÙˆØ³ÛŒ Ø´Ø§Ø¹Ø± Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø³Øª"
    assert punc_remover.fit_transform(input_text) == expected_output


def test_remove_redundant_characters():
    redundant_character_remover = RedundantCharacterRemover()
    input_text = "Ø³Ù„Ø§Ù…Ù…"
    expected_output = "Ø³Ù„Ø§Ù…Ù…"
    assert redundant_character_remover(input_text) == expected_output

    input_text = "Ø³Ù„Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…"
    expected_output = "Ø³Ù„Ø§Ù…Ù…"
    assert redundant_character_remover.fit_transform(input_text) == expected_output

    input_text = "Ø±ÙˆØ²ÛŒ Ø¨Ø§Øº Ø³Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø¨Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø² Ø¨ÙˆØ¯"
    expected_output = "Ø±ÙˆØ²ÛŒ Ø¨Ø§Øº Ø³Ø¨Ø² Ø¨ÙˆØ¯"
    assert redundant_character_remover(input_text) == expected_output


def test_remove_emojis():
    emoji_remover = EmojiRemover()
    input_text = "ğŸ˜ŠğŸ‡®ğŸ‡·Ø³Ù„Ø§Ù… Ú¯Ù„Ø§ÛŒ ØªÙˆ Ø®ÙˆÙ†Ù‡!ğŸ‰ğŸ‰ğŸŠğŸˆ"
    expected_output = "Ø³Ù„Ø§Ù… Ú¯Ù„Ø§ÛŒ ØªÙˆ Ø®ÙˆÙ†Ù‡!"
    assert emoji_remover(input_text) == expected_output

    input_text = "ğŸŒ¹Ø¨Ø§Ø² Ù‡Ù… Ù…Ø±Øº Ø³Ø­Ø±ğŸ” Ø¨Ø± Ø³Ø± Ù…Ù†Ø¨Ø± Ú¯Ù„"
    expected_output = "Ø¨Ø§Ø² Ù‡Ù… Ù…Ø±Øº Ø³Ø­Ø± Ø¨Ø± Ø³Ø± Ù…Ù†Ø¨Ø± Ú¯Ù„"

    assert emoji_remover.fit_transform(input_text) == expected_output


def test_remove_diacritics():
    diacritics_remover = DiacriticsRemover()
    input_text = "Ù…ÙÙ†Ù’"
    expected_output = "Ù…Ù†"
    assert diacritics_remover(input_text) == expected_output

    input_text = "Ú©ÙØ¬Ø§ Ù†ÙØ´Ø§Ù†Ù Ù‚ÙØ¯ÙÙ… Ù†Ø§ØªÙÙ…Ø§Ù… Ø®ÙˆØ§Ù‡ÙØ¯ Ù…Ø§Ù†Ø¯ØŸ"
    expected_output = "Ú©Ø¬Ø§ Ù†Ø´Ø§Ù† Ù‚Ø¯Ù… Ù†Ø§ØªÙ…Ø§Ù… Ø®ÙˆØ§Ù‡Ø¯ Ù…Ø§Ù†Ø¯ØŸ"
    assert diacritics_remover.fit_transform(input_text) == expected_output


def test_remove_stopwords():
    stopword_remover = StopWordRemover()
    input_text = "Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡Ù” Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª"
    expected_output = "Ø¬Ù…Ù„Ù‡Ù” Ù†Ù…ÙˆÙ†Ù‡"
    assert stopword_remover(input_text) == expected_output

    input_text = "ÙˆÛŒ Ø®Ø§Ø·Ø±Ù†Ø´Ø§Ù† Ú©Ø±Ø¯"
    expected_output = ""
    assert stopword_remover(input_text) == expected_output

    input_text = "Ø¨Ù‡ØªØ± Ø§Ø² Ø§ÛŒØ±Ø§Ù† Ú©Ø¬Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨ÙˆØ¯"
    expected_output = "Ø§ÛŒØ±Ø§Ù†"
    assert stopword_remover(input_text) == expected_output


def test_remove_non_persian():
    non_persian_remover = NonPersianRemover()
    input_text = "Ø¨Ø§ ÛŒÙ‡ Ú¯Ù„ Ø¨Ù‡Ø§Ø± Ù†Ù…ÛŒâ€ŒØ´Ù‡"
    expected_output = "Ø¨Ø§ ÛŒÙ‡ Ú¯Ù„ Ø¨Ù‡Ø§Ø± Ù†Ù…ÛŒâ€ŒØ´Ù‡"
    assert non_persian_remover(input_text) == expected_output

    input_text = "What you seek is seeking you!"
    expected_output = "     !"
    assert non_persian_remover(input_text) == expected_output

    input_text = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§Ù‹ panic attack Ú©Ø±Ø¯Ù…!"
    expected_output = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§   Ú©Ø±Ø¯Ù…!"
    assert non_persian_remover(input_text) == expected_output

    non_persian_remover = NonPersianRemover(keep_english=True)

    input_text = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§Ù‹ panic attack Ú©Ø±Ø¯Ù…!"
    expected_output = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§ panic attack Ú©Ø±Ø¯Ù…!"
    assert non_persian_remover(input_text) == expected_output

    input_text = "100 Ø³Ø§Ù„ Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø§Ù„â€ŒÙ‡Ø§"
    expected_output = "100 Ø³Ø§Ù„ Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø§Ù„â€ŒÙ‡Ø§"
    assert non_persian_remover(input_text) == expected_output

    non_persian_remover = NonPersianRemover(keep_diacritics=True)
    input_text = "Ú¯ÙÙ„Ù Ù…ÙÙ†Ùˆ Ø§ÙØ°ÛŒÙØª Ù†ÙÚ©ÙÙ†ÛŒÙ†!"
    expected_output = "Ú¯ÙÙ„Ù Ù…ÙÙ†Ùˆ Ø§ÙØ°ÛŒÙØª Ù†ÙÚ©ÙÙ†ÛŒÙ†!"
    assert non_persian_remover(input_text) == expected_output


def test_remove_html_tags():
    html_tag_remover = HTMLTagRemover(replace_with="")
    input_text = "<p>Ú¯Ù„ ØµØ¯Ø¨Ø±Ú¯ Ø¨Ù‡ Ù¾ÛŒØ´ ØªÙˆ ÙØ±Ùˆ Ø±ÛŒØ®Øª Ø² Ø®Ø¬Ù„Øª!</p>"
    expected_output = "Ú¯Ù„ ØµØ¯Ø¨Ø±Ú¯ Ø¨Ù‡ Ù¾ÛŒØ´ ØªÙˆ ÙØ±Ùˆ Ø±ÛŒØ®Øª Ø² Ø®Ø¬Ù„Øª!"
    assert html_tag_remover(input_text) == expected_output

    input_text = "<div>Ø¢Ù†Ø¬Ø§ Ø¨Ø¨Ø± Ù…Ø±Ø§ Ú©Ù‡ Ø´Ø±Ø§Ø¨Ù… Ù†Ù…ÛŒâ€ŒØ¨Ø±Ø¯!</div>"
    expected_output = "Ø¢Ù†Ø¬Ø§ Ø¨Ø¨Ø± Ù…Ø±Ø§ Ú©Ù‡ Ø´Ø±Ø§Ø¨Ù… Ù†Ù…ÛŒâ€ŒØ¨Ø±Ø¯!"
    assert html_tag_remover.fit_transform(input_text) == expected_output

    input_text = "<a href='https://example.com'>Example</a>"
    expected_output = "Example"
    assert html_tag_remover(input_text) == expected_output

    input_text = "Ø®Ø¯Ø§ÛŒØ§! Ø®Ø¯Ø§ÛŒØ§ØŒ <b>Ú©ÙˆÛŒØ±Ù…!</b>"
    result = html_tag_remover(input_text)
    assert result == "Ø®Ø¯Ø§ÛŒØ§! Ø®Ø¯Ø§ÛŒØ§ØŒ Ú©ÙˆÛŒØ±Ù…!"


def test_punctuation_spacings():
    batch_input = []
    batch_expected_output = []
    punct_space_standardizer = PunctuationSpacingStandardizer()
    input_text = "Ø³Ù„Ø§Ù…!Ú†Ø·ÙˆØ±ÛŒØŸ"
    expected_output = "Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ±ÛŒØŸ "
    assert punct_space_standardizer(input_text) == expected_output

    batch_input.append(input_text)
    batch_expected_output.append(expected_output)

    input_text = "Ø´Ø±Ú©Øª Â« Ú¯ÙˆÚ¯Ù„ Â»Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ ."
    expected_output = "Ø´Ø±Ú©Øª Â«Ú¯ÙˆÚ¯Ù„Â» Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯. "

    assert punct_space_standardizer.fit_transform(input_text) == expected_output

    batch_input.append(input_text)
    batch_expected_output.append(expected_output)

    assert list(punct_space_standardizer(batch_input)) == batch_expected_output
    assert (
        list(punct_space_standardizer.fit_transform(batch_input))
        == batch_expected_output
    )


def test_mention_remover():
    mention_remover = MentionRemover(replace_with="")
    input_text = "@user Ø´Ù…Ø§ Ø®Ø¨Ø± Ø¯Ø§Ø±ÛŒØ¯ØŸ"
    expected_output = " Ø´Ù…Ø§ Ø®Ø¨Ø± Ø¯Ø§Ø±ÛŒØ¯ØŸ"
    assert mention_remover(input_text) == expected_output

    input_text = "@user Ø³Ù„Ø§Ù… Ø±ÙÙ‚Ø§ @user"
    expected_output = " Ø³Ù„Ø§Ù… Ø±ÙÙ‚Ø§ "
    assert mention_remover.fit_transform(input_text) == expected_output


def test_hashtag_remover():
    hashtag_remover = HashtagRemover(replace_with="")
    input_text = "#Ù¾ÛŒØ´Ø±ÙØª_Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ ØªÙˆØ³Ø¹Ù‡"
    expected_output = " Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ ØªÙˆØ³Ø¹Ù‡"
    assert hashtag_remover(input_text) == expected_output

    input_text = "Ø±ÙˆØ² #Ú©ÙˆØ¯Ú© Ø´Ø§Ø¯ Ø¨Ø§Ø¯."
    expected_output = "Ø±ÙˆØ²  Ø´Ø§Ø¯ Ø¨Ø§Ø¯."
    assert hashtag_remover.fit_transform(input_text) == expected_output
