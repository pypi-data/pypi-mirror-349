{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e302f19c-9a05-488b-87bc-d0e5b8856a46",
   "metadata": {},
   "source": [
    "# Making ceci yaml files for SOMPZ\n",
    "\n",
    "Author: Sam Schmidt<br>\n",
    "Last Successfully run: May 6, 2025<br>\n",
    "\n",
    "This notebook will quickly demonstrate using rail pipelines infrastructure to quickly set up the yaml files needed to run SOMPZ on the command line.  We'll start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd07738-46b6-49ab-95f9-8da450161428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import qp\n",
    "import os\n",
    "import tables_io\n",
    "from rail.core import common_params\n",
    "from rail.pipelines.estimation.estimate_sompz import EstimateSomPZPipeline\n",
    "from rail.pipelines.estimation.inform_sompz import InformSomPZPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944ec08-9dff-4cdb-a677-6312604f79e1",
   "metadata": {},
   "source": [
    "Next, we will grab the same data used in the `rail_sompz_inform_demo-romandesc.ipynb` notebook.  If you already ran that notebook, then the data should already exist in the `DEMODATA` subdirectory and you can comment out the cell below or skip it.  If you need to grab the data, then the cell below will grab the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261f577-3ae1-4296-96b4-ddbc8cb5aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -O https://portal.nersc.gov/cfs/lsst/PZ/roman_desc_demo_data.tar.gz\n",
    "#!mkdir DEMODATA\n",
    "#!tar -xzvf roman_desc_demo_data.tar.gz\n",
    "#!mv romandesc*.hdf5 DEMODATA/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68172fb7-79de-4ae0-8f61-36e7b1e9eeea",
   "metadata": {},
   "source": [
    "This data uses a different `hdf5_groupname` than the default \"photometry\" value, so first we can redefine `hdf5_groupname` in the `COMMON_PARAMS` that are shared by all of RAIL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4e381-8cb7-4349-aa1b-79e508d63ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params.set_param_defaults(hdf5_groupname=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffee052-fd90-4343-90ca-ec2eb530ab43",
   "metadata": {},
   "source": [
    "We need to specify the locations of our input data files, we'll use the small files that we just downloaded from NERSC with 1850 spec objects, 3700 deep objects, and 5000 wide objects drawn from the Roman-Rubin sims:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52459a0-3c2d-40c2-b8ab-e56845436d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "specfile = \"DEMODATA/romandesc_spec_data_18c_noinf.hdf5\"\n",
    "deepfile = \"DEMODATA/romandesc_deep_data_37c_noinf.hdf5\"\n",
    "widefile = \"DEMODATA/romandesc_wide_data_50c_noinf.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d85fce-2c44-4b57-a956-1cf0a9ea1ef4",
   "metadata": {},
   "source": [
    "`InformSomPZPipeline` is initialized with a dictionary where the deep and wide filenames are associated with `input_deep_data` and `input_wide_data`, so we need to create a dictionary and feed it to the pipeline, then initialize an instance of this pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966710d1-6609-48b6-a356-3c801dfb2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_dict = dict(input_deep_data=deepfile, input_wide_data=widefile, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05c12b-4c11-4b3f-b53b-87cf1b7ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_pipe = InformSomPZPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105b54b-a660-4d04-bfb5-30b367dbdb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_pipe.initialize(\n",
    "    test_input_dict, \n",
    "    dict(\n",
    "        output_dir=\".\",\n",
    "        log_dir=\".\",\n",
    "        resume=True,\n",
    "    ),\n",
    "    None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e38caa-f97f-4a0c-bbbc-558abcc37ddb",
   "metadata": {},
   "source": [
    "That's all we need in order to create the yaml file! We can write the pair of yaml files to disk with inform_pipe.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014bab7-c45b-48b4-89ed-9f7e7393329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_pipe.save(\"inform_pipeline.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc70398-ebbd-4de4-936c-fdffe99d63e8",
   "metadata": {},
   "source": [
    "We can then run this pipeline from the command-line with the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723a4e3-be5c-4f92-853c-ef7a0fd14e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ceci inform_pipeline.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19731b6-7520-44e0-a619-c67c7a51ee72",
   "metadata": {},
   "source": [
    "This should run our two stages that create the wide and deep SOMs and generate two pickle files.<br>\n",
    "\n",
    "These two pickle files are inputs to the `EstimateSomPZPipeline`, we can set up an instance of that and initialize just like we did the inform pipeline.  In this case, we need to specify the following inputs in a dictionary: `wide_model` and `deep_model` (the wide and deep SOM pickle files created just now), and the filenames for the spec, deep, and wide data, `input_spec_data`, `input_deep_data`, and `input_wide_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b49870-2659-4ac4-a942-ff9a71103e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_pipeline = EstimateSomPZPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674c169-8189-4b81-b3cb-c1486ffe7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_dict={\n",
    "    'wide_model':\"model_som_informer_wide.pkl\",\n",
    "    'deep_model':\"model_som_informer_deep.pkl\",\n",
    "    'input_spec_data':specfile,\n",
    "    'input_deep_data':deepfile,\n",
    "    'input_wide_data':widefile,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f270cb4-7aaa-4795-ab65-08958f17f112",
   "metadata": {},
   "source": [
    "We initialize the pipeline in the same way as before, and again write out the yaml files with an `estimate_pipeline.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb48cda-120f-4d3e-89d4-d24b75e92974",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_pipeline.initialize(\n",
    "    estimate_dict, \n",
    "    dict(\n",
    "        output_dir=\".\",\n",
    "        log_dir=\".\",\n",
    "        resume=True,\n",
    "    ),\n",
    "    None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6b029-a72a-499a-b024-dcdcae0c1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_pipeline.save(\"estimate_pipeline.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b7418-8b42-485c-a1cf-35879323cd42",
   "metadata": {},
   "source": [
    "Let's run this via the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fe348-2ed5-4046-8438-45c79344724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ceci estimate_pipeline.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7918be-57d3-47da-8b0c-503a352cc88e",
   "metadata": {},
   "source": [
    "This pipeline runs multiple stages, and should create multiple intermediate files.  You can read a bit more about them in the `rail_sompz_estimate_demo_romandesc.ipynb` notebook, but we will simply look at the final output, the tomographic bins stored in `nz_som_nz.hdf5`.  This dataset should be a qp ensemble with four tomographic redshift bins.  Let's plot the four bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b21978-2a26-4c64-aa0a-f9306c403bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzfile = \"nz_som_nz.hdf5\"\n",
    "ens = qp.read(nzfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00ce41-fe3b-455a-aeb1-e483bb15b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.npdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2de69-5728-439c-8713-50d1dcaabddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10,8))\n",
    "for i in range(4):\n",
    "    ens.plot_native(key=i, axes=axs)\n",
    "axs.set_xlabel(\"redshift\", fontsize=14)\n",
    "axs.set_ylabel(\"N(z)\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d57ff-e697-4ff6-ab77-f389056dd7b3",
   "metadata": {},
   "source": [
    "We see four fairly well defined bins, exactly as expected for the small samples used in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a74b39-9af7-4681-89fe-574329978479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
