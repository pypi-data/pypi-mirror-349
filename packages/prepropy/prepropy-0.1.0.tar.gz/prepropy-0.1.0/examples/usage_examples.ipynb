{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f1f923",
   "metadata": {},
   "source": [
    "# PreProPy: Data Preprocessing Made Simple\n",
    "\n",
    "This notebook demonstrates the key features of the PreProPy package, which combines three essential data preprocessing tools:\n",
    "\n",
    "1. **NullSense**: Intelligent handling of missing values\n",
    "2. **DupliChecker**: Duplicate record detection and removal\n",
    "3. **ScaleNPipe**: Feature scaling and model pipeline creation\n",
    "\n",
    "Let's explore how to use these tools in your data science workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import PreProPy functions\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from prepropy import handle_nulls, drop_duplicates, get_duplicate_stats, scale_pipeline, get_available_scalers\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5fdc0",
   "metadata": {},
   "source": [
    "## Loading Sample Data\n",
    "\n",
    "First, let's load the sample dataset we generated earlier. This dataset has intentionally introduced missing values and duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2194f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "df = pd.read_csv('../sample_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f737961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isna().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_values.index, y=missing_values.values)\n",
    "plt.title('Missing Values by Column')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3b9db",
   "metadata": {},
   "source": [
    "## 1. NullSense: Handling Missing Values\n",
    "\n",
    "NullSense intelligently fills missing values based on column types. For numeric columns, it uses statistical measures like mean or median, and for categorical columns, it uses the mode (most frequent value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different strategies for handling missing values\n",
    "strategies = ['auto', 'mean', 'median', 'mode', 'zero']\n",
    "\n",
    "# Create a copy of the dataframe with selected columns for demonstration\n",
    "demo_df = df[['age', 'income', 'education', 'satisfaction']].copy()\n",
    "\n",
    "# Function to count missing values in a dataframe\n",
    "def count_missing(df):\n",
    "    return df.isna().sum().sum()\n",
    "\n",
    "# Demonstrate different strategies\n",
    "results = {}\n",
    "for strategy in strategies:\n",
    "    filled_df = handle_nulls(demo_df, strategy=strategy)\n",
    "    results[strategy] = filled_df.copy()\n",
    "    print(f\"Strategy: {strategy}, Missing values after: {count_missing(filled_df)}\")\n",
    "\n",
    "    # For demonstration, show first 5 rows of two columns\n",
    "    if strategy == 'auto':\n",
    "        print(\"\\nSample of filled values with 'auto' strategy:\")\n",
    "        print(\"\\nOriginal:\")\n",
    "        print(demo_df[['age', 'education']].head(5))\n",
    "        print(\"\\nFilled:\")\n",
    "        print(filled_df[['age', 'education']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c78097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution before and after filling missing values\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(demo_df['age'].dropna(), kde=True)\n",
    "plt.title('Age Distribution (Before)')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(results['auto']['age'], kde=True)\n",
    "plt.title('Age Distribution (After Auto Strategy)')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(demo_df['income'].dropna(), kde=True)\n",
    "plt.title('Income Distribution (Before)')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(results['auto']['income'], kde=True)\n",
    "plt.title('Income Distribution (After Auto Strategy)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29163fc",
   "metadata": {},
   "source": [
    "### Using the Auto Strategy\n",
    "\n",
    "The `auto` strategy is particularly useful because it applies different filling methods based on column type:\n",
    "- For numeric columns: uses median (more robust to outliers)\n",
    "- For categorical columns: uses mode (most frequent value)\n",
    "\n",
    "Let's proceed with the auto-filled dataset for the rest of our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the auto strategy for our main dataset\n",
    "df_filled = handle_nulls(df, strategy='auto')\n",
    "\n",
    "# Confirm no missing values remain\n",
    "print(f\"Missing values after filling: {count_missing(df_filled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806e85f",
   "metadata": {},
   "source": [
    "## 2. DupliChecker: Handling Duplicate Records\n",
    "\n",
    "DupliChecker helps identify and remove duplicate records from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21302671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics about duplicates in the dataset\n",
    "# We'll exclude 'id' since it's a unique identifier\n",
    "duplicate_stats = get_duplicate_stats(df_filled, subset=df_filled.columns[1:])\n",
    "\n",
    "print(\"Duplicate Statistics:\")\n",
    "for key, value in duplicate_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Visualize duplicate percentage\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Unique Records', 'Duplicate Records']\n",
    "sizes = [duplicate_stats['unique_count'], duplicate_stats['duplicate_count']]\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('Duplicate Records in Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49955de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different approaches to handling duplicates\n",
    "print(\"Original dataset shape:\", df_filled.shape)\n",
    "\n",
    "# Drop duplicates considering all non-id columns\n",
    "df_no_dups = drop_duplicates(df_filled, subset=df_filled.columns[1:])\n",
    "print(\"After dropping duplicates:\", df_no_dups.shape)\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "df_no_dups_subset = drop_duplicates(df_filled, subset=['job_category', 'education'])\n",
    "print(\"After dropping duplicates based on job_category and education:\", df_no_dups_subset.shape)\n",
    "\n",
    "# Keep last occurrence instead of first\n",
    "df_no_dups_last = drop_duplicates(df_filled, subset=df_filled.columns[1:], keep='last')\n",
    "print(\"After dropping duplicates (keeping last occurrence):\", df_no_dups_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d487da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find and examine a duplicate\n",
    "duplicated_rows = df_filled[df_filled.duplicated(subset=df_filled.columns[1:], keep=False)]\n",
    "print(f\"Found {len(duplicated_rows)} rows that are duplicates of others\")\n",
    "\n",
    "if len(duplicated_rows) > 0:\n",
    "    # Find an example duplicate pair\n",
    "    dup_values = duplicated_rows.iloc[0][df_filled.columns[1:]].values\n",
    "    example_dups = df_filled[\n",
    "        (df_filled[df_filled.columns[1:]] == dup_values).all(axis=1)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nExample of duplicate records:\")\n",
    "    print(example_dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13f147",
   "metadata": {},
   "source": [
    "For the rest of our analysis, we'll use the dataset with duplicates removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de323b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataset with duplicates removed\n",
    "df_clean = df_no_dups.copy()\n",
    "print(f\"Clean dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715542e4",
   "metadata": {},
   "source": [
    "## 3. ScaleNPipe: Feature Scaling and Model Pipeline\n",
    "\n",
    "ScaleNPipe helps create scikit-learn pipelines with feature scaling followed by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare a simple example for binary classification\n",
    "# We'll predict if a person's income is above the median\n",
    "median_income = df_clean['income'].median()\n",
    "df_clean['high_income'] = (df_clean['income'] > median_income).astype(int)\n",
    "\n",
    "# Select features and target\n",
    "features = ['age', 'satisfaction', 'years_experience']\n",
    "X = df_clean[features].values\n",
    "y = df_clean['high_income'].values\n",
    "\n",
    "# Check the available scalers\n",
    "scalers = get_available_scalers()\n",
    "print(\"Available scalers:\")\n",
    "for name, desc in scalers.items():\n",
    "    print(f\"- {name}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40571881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create and compare pipelines with different scalers\n",
    "scaler_types = ['standard', 'minmax', 'robust']\n",
    "cv_results = {}\n",
    "\n",
    "for scaler_type in scaler_types:\n",
    "    # Create pipeline\n",
    "    pipeline = scale_pipeline(model, scaler=scaler_type)\n",
    "    \n",
    "    # Cross-validation\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    cv_results[scaler_type] = scores\n",
    "    \n",
    "    print(f\"{scaler_type} scaler - Mean CV accuracy: {scores.mean():.4f}, std: {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff42ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "data_to_plot = [cv_results[scaler] for scaler in scaler_types]\n",
    "plt.boxplot(data_to_plot, labels=scaler_types)\n",
    "\n",
    "plt.title('Cross-Validation Accuracy with Different Scalers')\n",
    "plt.xlabel('Scaler Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10208e6",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "Let's see how different scaling methods affect the feature coefficients of our logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pipelines with different scalers and analyze feature coefficients\n",
    "feature_coefficients = {}\n",
    "\n",
    "for scaler_type in scaler_types:\n",
    "    # Create and train pipeline\n",
    "    pipeline = scale_pipeline(model, scaler=scaler_type)\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    # Extract coefficients\n",
    "    coefficients = pipeline.named_steps['model'].coef_[0]\n",
    "    feature_coefficients[scaler_type] = coefficients\n",
    "    \n",
    "    print(f\"\\n{scaler_type.capitalize()} Scaler - Feature Coefficients:\")\n",
    "    for feature, coef in zip(features, coefficients):\n",
    "        print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9410c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(features))\n",
    "width = 0.25\n",
    "\n",
    "for i, scaler_type in enumerate(scaler_types):\n",
    "    plt.bar(x + (i-1)*width, feature_coefficients[scaler_type], width, label=scaler_type)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Feature Coefficients with Different Scalers')\n",
    "plt.xticks(x, features)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379b64c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the three main components of the PreProPy package:\n",
    "\n",
    "1. **NullSense**: We filled missing values using different strategies, with the 'auto' strategy being particularly useful as it applies appropriate methods based on column types.\n",
    "\n",
    "2. **DupliChecker**: We identified and removed duplicate records, with options for which duplicates to keep and which columns to consider.\n",
    "\n",
    "3. **ScaleNPipe**: We created scikit-learn pipelines with different scalers, showing how scaling can affect model performance and feature coefficients.\n",
    "\n",
    "These tools can be easily incorporated into your data science workflow to streamline the preprocessing phase of your projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
